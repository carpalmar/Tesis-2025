# -*- coding: utf-8 -*-
"""NBEATSx h=1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DJMvBq45DP8_pQVqV0VGUojuqcemMcQR
"""

# ============================================================================
# NBEATSx h=1 - USD/PEN FORECASTING (CORRECTED VERSION)
# ============================================================================
# Tesis de Maestr√≠a - Forecasting FX con Deep Learning
# Dataset: DATA.csv (8201 obs, variables macro/t√©cnicas)
#
# CAMBIOS vs h=5:
# - h=1 (horizonte 1 d√≠a) para comparaci√≥n con ARX/NHITS
# - Solo identity stacks (trend/seasonality requieren h>1)
# - Evaluaci√≥n holdout SIN cross_validation (previene leakage)
# - Tuning reducido (50-100 trials, ~1-2h)
# ============================================================================

# ============================================================================
# CELDA 0: SETUP GOOGLE COLAB PRO+
# ============================================================================

print("="*80)
print("CONFIGURACI√ìN INICIAL - GOOGLE COLAB PRO+")
print("="*80)

# 1. Verificar GPU
import torch
print(f"\n1. GPU CONFIGURATION:")
print(f"   CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"   GPU: {torch.cuda.get_device_name(0)}")
    print(f"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    print(f"   ‚úì GPU READY")
else:
    print("   ‚ö†Ô∏è WARNING: No GPU detected. Go to Runtime ‚Üí Change runtime type")

# 2. Instalar paquetes faltantes
print(f"\n2. INSTALLING PACKAGES:")
print("   Installing neuralforecast...")
!pip install -q neuralforecast==1.7.4

print("   Installing optuna...")
!pip install -q optuna==3.6.1

print("   Installing arch (GARCH models)...")
!pip install -q arch==7.1.0

print("   ‚úì PACKAGES INSTALLED")

# 3. Montar Google Drive (para guardar outputs)
print(f"\n3. MOUNTING GOOGLE DRIVE:")
from google.colab import drive
drive.mount('/content/drive', force_remount=False)
print("   ‚úì DRIVE MOUNTED")

# 4. Crear directorio de outputs en Drive
import os
output_path = '/content/drive/MyDrive/Colab_Outputs/NBEATSx_h1_USD_PEN'
os.makedirs(output_path, exist_ok=True)
print(f"   Output directory: {output_path}")
print("   ‚úì OUTPUT DIR CREATED")
oof_path = '/content/drive/MyDrive/Colab_Outputs/oof_predictions'
os.makedirs(oof_path, exist_ok=True)
print(f"   OOF directory: {oof_path}")

# 5. Verificar versiones cr√≠ticas
print(f"\n4. PACKAGE VERSIONS:")
import pandas as pd
import numpy as np
print(f"   Python: 3.10+")
print(f"   Pandas: {pd.__version__}")
print(f"   NumPy: {np.__version__}")
print(f"   PyTorch: {torch.__version__}")

# Verificar NeuralForecast
try:
    import neuralforecast
    print(f"   NeuralForecast: {neuralforecast.__version__}")
    print("   ‚úì NEURALFORECAST OK")
except:
    print("   ‚ö†Ô∏è NeuralForecast installation failed")

# Verificar Optuna
try:
    import optuna
    print(f"   Optuna: {optuna.__version__}")
    print("   ‚úì OPTUNA OK")
except:
    print("   ‚ö†Ô∏è Optuna installation failed")

# Verificar ARCH
try:
    import arch
    print(f"   ARCH: {arch.__version__}")
    print("   ‚úì ARCH OK")
except:
    print("   ‚ö†Ô∏è ARCH installation failed")

print("\n" + "="*80)
print("‚úì SETUP COMPLETADO - READY TO START")
print("="*80)

# ============================================================================
# CELDA 0.1: Keep Colab Alive (evitar timeout 90 min)
# ============================================================================

from IPython.display import Javascript
display(Javascript('''
  function KeepAlive() {
    console.log("Keep alive ping");
    setTimeout(KeepAlive, 60000); // Cada 60 segundos
  }
  KeepAlive();
'''))

print("‚úì Keep-alive script activado (ping cada 60s)")

# CELDA 0.5: UPLOAD DATA.csv
from google.colab import files
uploaded = files.upload()
# Seleccionar DATA.csv desde tu computadora

# Verificar
import pandas as pd
df_test = pd.read_csv('DATA.csv')
print(f"‚úì DATA.csv loaded: {df_test.shape}")
print(f"Columns: {df_test.columns.tolist()}")

# ============================================================================
# CELDA 1: IMPORTS Y CONFIGURACI√ìN GLOBAL
# ============================================================================

"""
NBEATSx h=1 Implementation for USD/PEN Forecasting
Tesis de Maestr√≠a - Forecasting FX con Deep Learning
Dataset: DATA.csv (8201 obs, variables macro/t√©cnicas)
"""

# Standard imports
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from pathlib import Path
import json
import warnings
warnings.filterwarnings('ignore')

# Sklearn imports
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error

# NeuralForecast imports
from neuralforecast import NeuralForecast
from neuralforecast.models import NBEATSx
from neuralforecast.losses.pytorch import MAE

# Optuna for hyperparameter tuning
import optuna
from optuna.samplers import TPESampler

# GARCH for volatility
from arch import arch_model

# Logging setup
import logging

# Output directory (en Google Drive)
OUTPUT_DIR = Path('/content/drive/MyDrive/Colab_Outputs/NBEATSx_h1_USD_PEN')
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
OOF_DIR = Path('/content/drive/MyDrive/Colab_Outputs/oof_predictions')
OOF_DIR.mkdir(parents=True, exist_ok=True)

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(OUTPUT_DIR / 'nbeatsx_h1_training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ============================================================================
# CONFIGURACI√ìN GLOBAL
# ============================================================================

# Random seed for reproducibility
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

# ‚úÖ h=1 (ONE-STEP-AHEAD)
H_FORECAST = 1           # Predecir 1 d√≠a adelante

# Split parameters
MIN_TRAIN = 252          # 1 a√±o m√≠nimo
STEP_SIZE = 21           # Step de 1 mes (21 d√≠as trading)
N_HOLDOUT = 60           # 60 d√≠as holdout (~3 meses)

# Optuna tuning (REDUCIDO para h=1)
N_TRIALS = 75            # 75 trials (suficiente para h=1)
TIMEOUT = 5400           # 1.5 horas m√°ximo

# Baselines (COMPARABLES ahora - todos h=1)
BASELINE_ARX = {
    'modelo': 'AR(21) + UST10Y_diff_lag_1 + RSI',
    'MASE': 0.9398,
    'DA': 51.67,
    'metodologia': 'Walk-forward 766 splits',
    'h': 1
}

BASELINE_NHITS = {
    'modelo': 'NHITS (pooling jer√°rquico)',
    'MASE': 1.0395,
    'DA': 53.66,
    'best_exog': 'NONE',
    'metodologia': 'Walk-forward 187 splits',
    'h': 1
}

logger.info("="*80)
logger.info("NBEATSx h=1 USD/PEN FORECASTING - IMPLEMENTATION")
logger.info("="*80)
logger.info(f"Horizonte: h={H_FORECAST} d√≠a (ONE-STEP-AHEAD)")
logger.info(f"Random State: {RANDOM_STATE}")
logger.info(f"MIN_TRAIN: {MIN_TRAIN} | STEP_SIZE: {STEP_SIZE} | N_HOLDOUT: {N_HOLDOUT}")
logger.info(f"Optuna: {N_TRIALS} trials, timeout={TIMEOUT}s (~{TIMEOUT/3600:.1f}h)")
logger.info(f"‚ö†Ô∏è NOTA: h=1 solo permite identity stacks (trend/seasonality no disponibles)")
logger.info(f"Output Directory: {OUTPUT_DIR}")

print(f"‚úì Celda 1 completada: Configuraci√≥n h={H_FORECAST} cargada")

# ============================================================================
# CELDA 2: FXMetrics (SIN CAMBIOS)
# ============================================================================

class FXMetrics:
    """
    M√©tricas de evaluaci√≥n para forecasting USD/PEN

    M√©tricas:
        - DA (Directional Accuracy): % predicciones direccionales correctas
        - MASE (Mean Absolute Scaled Error): Error vs naive benchmark
        - MAE (Mean Absolute Error): Error absoluto promedio
    """

    @staticmethod
    def levels_to_returns(y_level: np.ndarray, y_prev_level: np.ndarray) -> np.ndarray:
        """Convierte niveles de precio a log-returns"""
        return np.log(y_level / y_prev_level)

    @staticmethod
    def directional_accuracy(y_true_ret: np.ndarray, y_pred_ret: np.ndarray) -> float:
        """Calcula Directional Accuracy (DA)"""
        correct = np.sum(np.sign(y_true_ret) == np.sign(y_pred_ret))
        total = len(y_true_ret)
        da = (correct / total) * 100
        return da

    @staticmethod
    def mase(y_true_ret: np.ndarray,
             y_pred_ret: np.ndarray,
             y_train_ret: np.ndarray) -> float:
        """Calcula Mean Absolute Scaled Error (MASE)"""
        mae_model = np.mean(np.abs(y_true_ret - y_pred_ret))
        mae_naive = np.mean(np.abs(np.diff(y_train_ret)))

        if mae_naive == 0:
            logger.warning("MAE naive = 0, retornando MASE = inf")
            return np.inf

        mase_value = mae_model / mae_naive
        return mase_value

    @staticmethod
    def evaluate(y_true_level: np.ndarray,
                 y_pred_level: np.ndarray,
                 y_prev_level: np.ndarray,
                 y_train_returns: np.ndarray) -> dict:
        """Evaluaci√≥n completa de predicciones"""
        # Convertir niveles a returns
        y_true_ret = FXMetrics.levels_to_returns(y_true_level, y_prev_level)
        y_pred_ret = FXMetrics.levels_to_returns(y_pred_level, y_prev_level)

        # Calcular m√©tricas
        da = FXMetrics.directional_accuracy(y_true_ret, y_pred_ret)
        mase_value = FXMetrics.mase(y_true_ret, y_pred_ret, y_train_returns)
        mae_ret = np.mean(np.abs(y_true_ret - y_pred_ret))
        mae_level = np.mean(np.abs(y_true_level - y_pred_level))

        metrics = {
            'DA': da,
            'MASE': mase_value,
            'MAE_returns': mae_ret,
            'MAE_levels': mae_level
        }

        return metrics

    @staticmethod
    def compare_to_baseline(metrics: dict) -> dict:
        """Compara m√©tricas actuales vs ARX y NHITS baselines"""
        comparison = {
            'baseline_arx': BASELINE_ARX,
            'baseline_nhits': BASELINE_NHITS,
            'nbeatsx': {
                'MASE': metrics['MASE'],
                'DA': metrics['DA']
            },
            'delta_vs_arx': {
                'DA': metrics['DA'] - BASELINE_ARX['DA'],
                'MASE': metrics['MASE'] - BASELINE_ARX['MASE']
            },
            'delta_vs_nhits': {
                'DA': metrics['DA'] - BASELINE_NHITS['DA'],
                'MASE': metrics['MASE'] - BASELINE_NHITS['MASE']
            }
        }

        # Determinar veredicto
        if metrics['DA'] > BASELINE_NHITS['DA'] and metrics['MASE'] < BASELINE_NHITS['MASE']:
            verdict = "‚úì SUPERIOR - NBEATSx supera NHITS en ambas m√©tricas"
        elif metrics['DA'] > BASELINE_ARX['DA'] and metrics['MASE'] < BASELINE_ARX['MASE']:
            verdict = "‚úì SUPERIOR - NBEATSx supera ARX en ambas m√©tricas"
        elif metrics['DA'] > 55:
            verdict = "~ ACEPTABLE - DA>55% (mejora direccional sustancial)"
        elif metrics['DA'] > BASELINE_NHITS['DA']:
            verdict = "~ MIXTO - Mejor DA que NHITS pero peor/similar MASE"
        else:
            verdict = "‚úó INFERIOR - No supera baselines"

        comparison['verdict'] = verdict

        return comparison

logger.info("FXMetrics class loaded successfully")
logger.info(f"OOF Directory: {OOF_DIR}")
print("‚úì Celda 2 completada: FXMetrics cargada")

# ============================================================================
# FUNCI√ìN DE EXPORTACI√ìN PARA META-LEARNER
# ============================================================================

def export_model_predictions(dates, y_pred, model_name, prediction_type, output_dir='predictions_dump'):
    """
    Guarda las predicciones crudas para el Meta-Learner.

    Args:
        dates (array-like): Fechas del holdout (deben ser 60).
        y_pred (array-like): Valores predichos (Niveles o Retornos, seg√∫n el modelo).
        model_name (str): Nombre √∫nico (ej: 'CatBoost', 'NHITS').
        prediction_type (str): 'levels' o 'log_returns' (para saber qu√© transformaci√≥n aplicar luego).
        output_dir (str): Carpeta donde se guardar√°n los CSVs.
    """
    from pathlib import Path

    # Crear carpeta si no existe
    save_path = Path(output_dir)
    save_path.mkdir(parents=True, exist_ok=True)

    filename = save_path / f"pred_{model_name}.csv"

    # Crear DataFrame ligero
    df_export = pd.DataFrame({
        'ds': dates,
        'y_pred': y_pred,
        'model': model_name,
        'type': prediction_type
    })

    df_export.to_csv(filename, index=False)
    print(f"‚úÖ [EXPORT] Predicciones guardadas en: {filename}")
    print(f"   - Dimensiones: {df_export.shape}")
    print(f"   - Tipo: {prediction_type}")

logger.info("‚úì Funci√≥n export_model_predictions cargada")

# MODIFICACI√ìN 3: CELDA 2.5 (NUEVA) - Funci√≥n save_oof_predictions
# ============================================================================
# CREAR NUEVA CELDA despu√©s de CELDA 2 (FXMetrics) y antes de CELDA 3
# COPIAR TODO ESTE C√ìDIGO:
# ----------------------------------------------------------------------------

def save_oof_predictions(predictions, dates, actuals, model_name,
                         prediction_type, metadata, output_dir=OOF_DIR):
    """
    Guardar predicciones Out-of-Fold (OOF) para el meta-learner.

    Args:
        predictions: np.array con predicciones OOF
        dates: lista/array con fechas correspondientes
        actuals: np.array con valores reales
        model_name: nombre del modelo (ej: 'NBEATSx')
        prediction_type: 'log_returns' o 'levels'
        metadata: dict con informaci√≥n adicional
        output_dir: directorio de salida
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # Crear DataFrame OOF
    df_oof = pd.DataFrame({
        'ds': dates,
        'y_pred': predictions,
        'y_real': actuals,
        'model': model_name,
        'type': prediction_type
    })

    # Guardar CSV
    csv_path = output_dir / f'train_oof_{model_name}.csv'
    df_oof.to_csv(csv_path, index=False)

    # Guardar metadata
    metadata_path = output_dir / f'train_oof_{model_name}_metadata.json'
    with open(metadata_path, 'w') as f:
        json.dump(metadata, f, indent=2, default=str)

    print(f"\n{'='*60}")
    print(f"üíæ OOF PREDICTIONS GUARDADAS")
    print(f"{'='*60}")
    print(f"   Archivo CSV:  {csv_path}")
    print(f"   Metadata:     {metadata_path}")
    print(f"   Observaciones: {len(predictions)}")
    print(f"   Modelo:        {model_name}")
    print(f"   Tipo:          {prediction_type}")

    # Validaciones
    n_zeros = np.sum(np.array(predictions) == 0)
    n_nans = np.sum(np.isnan(predictions))

    if n_zeros > 0:
        print(f"   ‚ö†Ô∏è Zeros:     {n_zeros}")
    if n_nans > 0:
        print(f"   ‚ö†Ô∏è NaNs:      {n_nans}")

    return csv_path

logger.info("‚úì Funci√≥n save_oof_predictions cargada")
print("‚úì Celda 2.5 completada: save_oof_predictions cargada")

# ============================================================================
# CELDA 3: TechnicalFeatures (SIN CAMBIOS)
# ============================================================================

class TechnicalFeatures:
    """
    Generador de features t√©cnicos SIN DATA LEAKAGE

    CR√çTICO: Todos los features deben calcularse sobre data SHIFTED
    """

    @staticmethod
    def calculate_garch_volatility(returns: pd.Series, window: int = 252) -> pd.Series:
        """
        Calcula volatilidad GARCH(1,1) sin data leakage

        CR√çTICO: returns debe estar YA SHIFTED (returns.shift(1))
        """
        logger.info(f"Calculating GARCH volatility (window={window})")

        garch_vol = pd.Series(index=returns.index, dtype=float)
        fallback_count = 0

        for i in range(window, len(returns)):
            # Ventana de returns [t-window:t-1]
            ret_window = returns.iloc[i-window:i]

            # Eliminar NaNs
            ret_window = ret_window.dropna()

            if len(ret_window) < window * 0.8:  # Require 80% de datos
                garch_vol.iloc[i] = np.nan
                continue

            try:
                # Fit GARCH(1,1)
                model = arch_model(ret_window * 100, vol='Garch', p=1, q=1, rescale=False)
                res = model.fit(disp='off', show_warning=False)

                # Forecast 1 step ahead
                forecast = res.forecast(horizon=1, reindex=False)
                garch_vol.iloc[i] = np.sqrt(forecast.variance.values[-1, 0]) / 100

            except Exception as e:
                # Fallback to EWMA
                fallback_count += 1
                garch_vol.iloc[i] = ret_window.ewm(span=21).std().iloc[-1]

        if fallback_count > 0:
            logger.warning(f"GARCH fallback to EWMA in {fallback_count} points ({fallback_count/len(returns)*100:.1f}%)")

        return garch_vol

    @staticmethod
    def calculate_rsi(prices: pd.Series, period: int = 14) -> pd.Series:
        """Calcula RSI normalizado sin data leakage"""
        logger.info(f"Calculating RSI (period={period})")

        # Calcular cambios
        delta = prices.diff()

        # Separar ganancias y p√©rdidas
        gain = delta.where(delta > 0, 0)
        loss = -delta.where(delta < 0, 0)

        # EWMA de ganancias y p√©rdidas
        avg_gain = gain.ewm(span=period, adjust=False).mean()
        avg_loss = loss.ewm(span=period, adjust=False).mean()

        # RS y RSI
        rs = avg_gain / avg_loss
        rsi = 100 - (100 / (1 + rs))

        # Normalizar a [-1, 1]
        rsi_normalized = (rsi - 50) / 50

        return rsi_normalized

    @staticmethod
    def calculate_macd(prices: pd.Series, fast: int = 12, slow: int = 26) -> pd.Series:
        """Calcula MACD line normalizado sin data leakage"""
        logger.info(f"Calculating MACD (fast={fast}, slow={slow})")

        # EMAs
        ema_fast = prices.ewm(span=fast, adjust=False).mean()
        ema_slow = prices.ewm(span=slow, adjust=False).mean()

        # MACD line
        macd_line = ema_fast - ema_slow

        # Normalizar por precio (como % del precio)
        macd_normalized = macd_line / prices

        return macd_normalized

    @staticmethod
    def create_all_features(prices: pd.Series, returns: pd.Series) -> pd.DataFrame:
        """Pipeline completo de features t√©cnicos"""
        logger.info("Creating all technical features")

        features = pd.DataFrame(index=prices.index)

        # GARCH volatility
        features['GARCH_vol'] = TechnicalFeatures.calculate_garch_volatility(returns, window=252)

        # RSI
        features['RSI'] = TechnicalFeatures.calculate_rsi(prices, period=14)

        # MACD
        features['MACD_line'] = TechnicalFeatures.calculate_macd(prices, fast=12, slow=26)

        logger.info(f"Technical features created. Shape: {features.shape}")
        logger.info(f"NaN counts: {features.isna().sum().to_dict()}")

        return features

logger.info("TechnicalFeatures class loaded successfully")
print("‚úì Celda 3 completada: TechnicalFeatures cargada")

# ============================================================================
# CELDA 4: FeatureEngineer (SIN CAMBIOS)
# ============================================================================

class FeatureEngineer:
    """
    Feature engineering completo sin leakage

    Output:
        - PEN_target: Niveles (target para modelo)
        - PEN_lag_1: Para conversi√≥n a returns en evaluaci√≥n
        - all_exog_features: 35+ candidatas
    """

    def __init__(self):
        self.technical_features = TechnicalFeatures()

    def cyclical_encoding(self, df: pd.DataFrame, col: str, max_val: int) -> pd.DataFrame:
        """Encoding c√≠clico (sin/cos) para variables temporales"""
        df[f'{col}_sin'] = np.sin(2 * np.pi * df[col] / max_val)
        df[f'{col}_cos'] = np.cos(2 * np.pi * df[col] / max_val)
        return df

    def create_features(self, df_raw: pd.DataFrame) -> pd.DataFrame:
        """Feature engineering completo"""
        logger.info("="*80)
        logger.info("FEATURE ENGINEERING - INICIANDO")
        logger.info("="*80)

        df = df_raw.copy()

        # 1. Parse dates
        df['Dates'] = pd.to_datetime(df['Dates'])
        df = df.set_index('Dates')
        df = df.sort_index()

        logger.info(f"Raw data shape: {df.shape}")
        logger.info(f"Date range: {df.index.min()} to {df.index.max()}")

        # 2. Target (PEN en niveles, NO transformar)
        df['PEN_target'] = df['PEN']
        df['PEN_lag_1'] = df['PEN'].shift(1)  # Para conversi√≥n a returns

        # 3. Features t√©cnicos (ANTI-LEAKAGE: shift ANTES del c√°lculo)
        pen_shifted = df['PEN'].shift(1)
        pen_ret_shifted = np.log(pen_shifted / pen_shifted.shift(1))

        tech_features = self.technical_features.create_all_features(pen_shifted, pen_ret_shifted)
        df = pd.concat([df, tech_features], axis=1)

        # 4. Ex√≥genas macro/FX (log returns con lag_1)
        macro_cols = ['MXN', 'CLP', 'COBRE', 'MXPE']
        for col in macro_cols:
            if col in df.columns:
                df[f'{col}_ret'] = np.log(df[col] / df[col].shift(1))
                df[f'{col}_lag_1'] = df[f'{col}_ret'].shift(1)

        # UST10Y y DXY (diferencias con lag_1)
        if 'UST10Y' in df.columns:
            df['UST10Y_diff'] = df['UST10Y'].diff()
            df['UST10Y_lag_1'] = df['UST10Y_diff'].shift(1)

        if 'DXY' in df.columns:
            df['DXY_diff'] = df['DXY'].diff()
            df['DXY_lag_1'] = df['DXY_diff'].shift(1)

        # CPI, RESERV, T_TRADE, Tasa_cds (si existen)
        optional_cols = ['CPI', 'RESERV', 'T_TRADE', 'Tasa_cds']
        for col in optional_cols:
            if col in df.columns:
                df[f'{col}_diff'] = df[col].diff()
                df[f'{col}_lag_1'] = df[f'{col}_diff'].shift(1)

        # 5. Variables temporales (encoding c√≠clico)
        df['month'] = df.index.month
        df['day_of_week'] = df.index.dayofweek
        df['quarter'] = df.index.quarter

        df = self.cyclical_encoding(df, 'month', 12)
        df = self.cyclical_encoding(df, 'day_of_week', 7)
        df = self.cyclical_encoding(df, 'quarter', 4)

        # 6. Identificar todas las ex√≥genas
        all_exog_features = [
            # T√©cnicas (3)
            'GARCH_vol', 'RSI', 'MACD_line',
        ]

        # Agregar macro/FX si existen
        for col in macro_cols:
            if f'{col}_lag_1' in df.columns:
                all_exog_features.append(f'{col}_lag_1')

        if 'UST10Y_lag_1' in df.columns:
            all_exog_features.append('UST10Y_lag_1')
        if 'DXY_lag_1' in df.columns:
            all_exog_features.append('DXY_lag_1')

        # Agregar opcionales si existen
        for col in optional_cols:
            if f'{col}_lag_1' in df.columns:
                all_exog_features.append(f'{col}_lag_1')

        # Temporales (6)
        all_exog_features.extend([
            'month_sin', 'month_cos',
            'day_of_week_sin', 'day_of_week_cos',
            'quarter_sin', 'quarter_cos'
        ])

        # Verificar que existen
        all_exog_features = [col for col in all_exog_features if col in df.columns]

        logger.info(f"Total exogenous features: {len(all_exog_features)}")
        logger.info(f"Features: {all_exog_features}")

        # 7. Dropear filas con NaNs en target o features cr√≠ticas
        critical_cols = ['PEN_target', 'PEN_lag_1'] + all_exog_features
        df_clean = df[critical_cols].dropna()

        logger.info(f"After dropna: {df_clean.shape}")
        logger.info(f"Dropped rows: {len(df) - len(df_clean)}")

        # 8. Validaci√≥n anti-leakage
        self._validate_no_leakage(df_clean, all_exog_features)

        # Almacenar lista de ex√≥genas
        df_clean.attrs['all_exog_features'] = all_exog_features

        logger.info("FEATURE ENGINEERING - COMPLETADO")
        logger.info("="*80)

        return df_clean

    def _validate_no_leakage(self, df: pd.DataFrame, exog_features: list):
        """Validaci√≥n anti-leakage"""
        logger.info("Validating no data leakage...")

        # Check: Ninguna ex√≥gena debe correlacionar perfectamente con target actual
        for feat in exog_features:
            if feat in df.columns:
                corr = df['PEN_target'].corr(df[feat])
                if abs(corr) > 0.99:
                    logger.error(f"LEAKAGE DETECTED: {feat} has correlation {corr:.4f} with PEN_target")
                    raise ValueError(f"Data leakage in {feat}")

        logger.info("‚úì No data leakage detected")

logger.info("FeatureEngineer class loaded successfully")
print("‚úì Celda 4 completada: FeatureEngineer cargada")

# ============================================================================
# CELDA 5: NeuralForecastFormatter (SIN CAMBIOS)
# ============================================================================

class NeuralForecastFormatter:
    """
    Conversi√≥n wide ‚Üí long + scaling sin leakage

    Long format requerido por NeuralForecast:
        unique_id | ds | y | exog1 | exog2 | ...
    """

    def __init__(self):
        self.scaler = StandardScaler()
        self.exog_to_scale = []
        self.exog_no_scale = []
        self.fitted = False

    def format_to_long(self, df: pd.DataFrame, target_col: str, exog_features: list) -> pd.DataFrame:
        """
        Convierte DataFrame wide a formato long de NeuralForecast

        Args:
            df: DataFrame con target y ex√≥genas
            target_col: Columna target (PEN_target)
            exog_features: Lista de ex√≥genas

        Returns:
            DataFrame long con columnas: unique_id, ds, y, exog1, exog2, ...
        """
        logger.info("Converting to NeuralForecast long format")

        # Crear DataFrame long
        df_long = pd.DataFrame({
            'unique_id': 'PEN',  # Identificador √∫nico de serie
            'ds': df.index,      # Fechas
            'y': df[target_col].values  # Target
        })

        # Agregar ex√≥genas
        for exog in exog_features:
            if exog in df.columns:
                df_long[exog] = df[exog].values

        # Reset index
        df_long = df_long.reset_index(drop=True)

        logger.info(f"Long format shape: {df_long.shape}")
        logger.info(f"Columns: {df_long.columns.tolist()}")

        return df_long

    def fit_scaler(self, train_df: pd.DataFrame, exog_to_scale: list, exog_no_scale: list):
        """
        Fit scaler en train set (sin leakage)

        Args:
            train_df: DataFrame long de train
            exog_to_scale: Ex√≥genas que necesitan scaling (GARCH, macro)
            exog_no_scale: Ex√≥genas ya normalizadas (RSI, MACD, temporales)
        """
        logger.info("Fitting scaler on train set")

        self.exog_to_scale = exog_to_scale
        self.exog_no_scale = exog_no_scale

        if len(exog_to_scale) > 0:
            # Fit solo en ex√≥genas que necesitan scaling
            self.scaler.fit(train_df[exog_to_scale])
            self.fitted = True
            logger.info(f"Scaler fitted on {len(exog_to_scale)} features")
        else:
            logger.info("No features to scale")

    def transform_scaler(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Transform ex√≥genas con scaler (preservando no_scale)

        Args:
            df: DataFrame long (train o holdout)

        Returns:
            DataFrame con ex√≥genas escaladas
        """
        df_scaled = df.copy()

        if self.fitted and len(self.exog_to_scale) > 0:
            # Scale solo ex√≥genas que lo necesitan
            df_scaled[self.exog_to_scale] = self.scaler.transform(df[self.exog_to_scale])
            logger.info(f"Transformed {len(self.exog_to_scale)} features")

        return df_scaled

    def validate_long_format(self, df: pd.DataFrame):
        """Validar formato long correcto"""
        required_cols = ['unique_id', 'ds', 'y']

        for col in required_cols:
            assert col in df.columns, f"Missing required column: {col}"

        assert df['unique_id'].nunique() == 1, "Multiple unique_ids found"
        assert not df['y'].isna().any(), "NaNs in target"

        logger.info("‚úì Long format validated")

logger.info("NeuralForecastFormatter class loaded successfully")
print("‚úì Celda 5 completada: NeuralForecastFormatter cargada")

# ============================================================================
# CELDA 6: NBEATSxBuilder (h=1 - SOLO IDENTITY STACKS) - CORRECTED
# ============================================================================

class NBEATSxBuilder:
    """
    Builder para modelos NBEATSx con h=1

    ‚ö†Ô∏è LIMITACI√ìN h=1: Solo identity stacks disponibles
    (trend y seasonality requieren h>1)

    ‚úÖ CORRECCI√ìN: Para h=1, usar hist_exog_list (NO futr_exog_list)
    """

    @staticmethod
    def build_model(h: int,
                   input_size: int,
                   max_steps: int,
                   learning_rate: float,
                   stack_types: list,
                   n_blocks: list,
                   mlp_units_size: int,
                   batch_size: int,
                   hist_exog_list: list = None,  # ‚úÖ CAMBIO: hist_exog_list
                   val_check_steps: int = 50,
                   early_stop_patience_steps: int = -1) -> NBEATSx:
        """
        Construye modelo NBEATSx para h=1

        ‚úÖ IMPORTANTE: Para h=1, usar hist_exog_list (ex√≥genas hist√≥ricas)
                       NO usar futr_exog_list (causar√≠a leakage o errores)

        Args:
            h: Horizonte de predicci√≥n (1 d√≠a)
            input_size: Lookback window (63-252 d√≠as)
            max_steps: Training epochs (300-600)
            learning_rate: Learning rate
            stack_types: Lista de stacks (SOLO 'identity' para h=1)
            n_blocks: Bloques por stack
            mlp_units_size: Tama√±o de MLPs
            batch_size: Batch size
            hist_exog_list: Lista de ex√≥genas HIST√ìRICAS (lag_1)
            val_check_steps: Intervalo de validaci√≥n
            early_stop_patience_steps: Early stopping (-1 = disabled)

        Returns:
            Modelo NBEATSx configurado
        """
        # ‚ö†Ô∏è Para h=1, SOLO identity stacks
        if h == 1:
            invalid = [s for s in stack_types if s not in ['identity']]
            if invalid:
                logger.warning(
                    f"Stack types {invalid} no v√°lidos para h=1. "
                    f"Usando solo 'identity' stacks."
                )
                stack_types = ['identity'] * len(stack_types)

        # Construir mlp_units por stack
        n_stacks = len(stack_types)
        mlp_units = [[mlp_units_size, mlp_units_size]] * n_stacks

        # Validar y ajustar n_blocks
        if len(n_blocks) != n_stacks:
            logger.warning(
                f"n_blocks ({len(n_blocks)}) != n_stacks ({n_stacks}). "
                f"Ajustando n_blocks..."
            )
            if len(n_blocks) == 1:
                n_blocks = n_blocks * n_stacks
            elif len(n_blocks) > n_stacks:
                n_blocks = n_blocks[:n_stacks]
            else:
                n_blocks = n_blocks + [n_blocks[-1]] * (n_stacks - len(n_blocks))

            logger.info(f"Adjusted n_blocks: {n_blocks}")

        model = NBEATSx(
            h=h,
            input_size=input_size,
            max_steps=max_steps,
            learning_rate=learning_rate,

            # Arquitectura (SOLO identity para h=1)
            stack_types=stack_types,
            n_blocks=n_blocks,
            mlp_units=mlp_units,

            # ‚úÖ Ex√≥genas HIST√ìRICAS (NO futuras para h=1)
            hist_exog_list=hist_exog_list if hist_exog_list else [],
            futr_exog_list=[],  # ‚úÖ VAC√çO para h=1 (previene leakage)

            # Training
            batch_size=batch_size,
            scaler_type='standard',
            random_seed=RANDOM_STATE,
            loss=MAE(),

            # Validation
            val_check_steps=val_check_steps,
            early_stop_patience_steps=early_stop_patience_steps,

            # Regularizaci√≥n
            num_lr_decays=2
        )

        logger.info(
            f"NBEATSx construido: h={h}, {n_stacks} stacks {stack_types}, "
            f"n_blocks={n_blocks}, input_size={input_size}, "
            f"hist_exog={len(hist_exog_list) if hist_exog_list else 0}"
        )

        return model

    @staticmethod
    def get_optuna_search_space() -> dict:
        """
        Search space para h=1 (SOLO identity stacks)

        ‚ö†Ô∏è M√°s compacto que h=5 porque:
        - Solo identity (no trend/seasonality)
        - Menor diversidad arquitectural
        - Tuning m√°s r√°pido
        """
        search_space = {
            # Input sizes (3 meses - 1 a√±o)
            'input_size': [63, 126, 168, 252],

            # Training epochs
            'max_steps': [300, 400, 500, 600],

            # Learning rate range
            'learning_rate': [5e-5, 5e-3],

            # Stack configurations (SOLO identity)
            'stack_configs': [
                # Pure identity con diferentes profundidades
                (['identity'], [2]),
                (['identity'], [3]),
                (['identity'], [4]),
                (['identity'], [5]),

                # Dual identity stacks
                (['identity', 'identity'], [2, 2]),
                (['identity', 'identity'], [3, 3]),
                (['identity', 'identity'], [4, 4]),
                (['identity', 'identity'], [5, 5]),
                (['identity', 'identity'], [3, 4]),
                (['identity', 'identity'], [4, 3]),

                # Triple identity stacks
                (['identity', 'identity', 'identity'], [3, 3, 3]),
                (['identity', 'identity', 'identity'], [4, 4, 4]),
                (['identity', 'identity', 'identity'], [3, 4, 3])
            ],

            # MLPs (m√°s compactos para h=1)
            'mlp_units_size': [256, 512, 1024],

            # Batch sizes
            'batch_size': [16, 32, 64]
        }

        return search_space

logger.info("NBEATSxBuilder class loaded successfully (h=1 version - CORRECTED)")
print("‚úì Celda 6 completada: NBEATSxBuilder h=1 (hist_exog only)")

# ============================================================================
# CELDA 7: NBEATSxTuner (h=1 - DEBUG VERSION)
# ============================================================================

class NBEATSxTuner:
    """
    Hyperparameter tuner para h=1 con DEBUG logging
    """

    def __init__(self, h: int = H_FORECAST):
        self.builder = NBEATSxBuilder()
        self.h = h

    def _calculate_da_from_predictions(self, y_true_levels, y_pred_levels):
        """Calcula DA desde predicciones en niveles"""
        logger.info(f"      Computing DA from {len(y_true_levels)} predictions")

        if len(y_true_levels) < 10:
            logger.warning("      Too few predictions")
            return 0.0

        # Calcular returns
        y_true_returns = np.diff(np.log(y_true_levels))
        y_pred_returns = np.diff(np.log(y_pred_levels))

        logger.info(f"      Returns computed: true_mean={y_true_returns.mean():.6f}, "
                   f"pred_mean={y_pred_returns.mean():.6f}")

        # Validar
        if not (np.isfinite(y_true_returns).all() and np.isfinite(y_pred_returns).all()):
            logger.warning("      Non-finite returns detected")
            return 0.0

        if np.std(y_pred_returns) < 1e-6:
            logger.warning("      Predictions have near-zero variance")
            return 0.0

        # Direcciones
        dir_true = np.sign(y_true_returns)
        dir_pred = np.sign(y_pred_returns)

        # DA
        valid_mask = dir_true != 0
        if np.sum(valid_mask) < 5:
            logger.warning("      Too few non-zero directions")
            return 0.0

        correct = np.sum(dir_true[valid_mask] == dir_pred[valid_mask])
        total = np.sum(valid_mask)

        da = 100.0 * correct / total
        logger.info(f"      DA computed: {da:.2f}% ({correct}/{total} correct)")

        return da

    def _objective(self, trial, train_inner_df, valid_df, full_train_df, hist_exog_list):
        """
        Funci√≥n objetivo con DEBUG logging
        """
        logger.info(f"\n{'='*60}")
        logger.info(f"TRIAL #{trial.number} - STARTING")
        logger.info(f"{'='*60}")

        search_space = self.builder.get_optuna_search_space()

        # Sample hyperparameters
        input_size = trial.suggest_categorical('input_size', search_space['input_size'])
        max_steps = trial.suggest_categorical('max_steps', search_space['max_steps'])
        learning_rate = trial.suggest_float('learning_rate',
                                            search_space['learning_rate'][0],
                                            search_space['learning_rate'][1],
                                            log=True)

        stack_config = trial.suggest_categorical('stack_config',
                                                  [str(s) for s in search_space['stack_configs']])
        stack_config = eval(stack_config)
        stack_types = stack_config[0]
        n_blocks = stack_config[1]

        mlp_units_size = trial.suggest_categorical('mlp_units_size',
                                                   search_space['mlp_units_size'])
        batch_size = trial.suggest_categorical('batch_size',
                                               search_space['batch_size'])

        logger.info(f"  Config: input={input_size}, steps={max_steps}, lr={learning_rate:.5f}")
        logger.info(f"          stacks={stack_types}, blocks={n_blocks}")
        logger.info(f"          mlp={mlp_units_size}, batch={batch_size}")

        # Build model
        logger.info(f"  Building model...")
        try:
            model = self.builder.build_model(
                h=self.h,
                input_size=input_size,
                max_steps=max_steps,
                learning_rate=learning_rate,
                stack_types=stack_types,
                n_blocks=n_blocks,
                mlp_units_size=mlp_units_size,
                batch_size=batch_size,
                hist_exog_list=hist_exog_list,
                val_check_steps=50,
                early_stop_patience_steps=-1
            )
            logger.info(f"  ‚úì Model built")
        except Exception as e:
            logger.error(f"  ‚úó Model build failed: {e}")
            return 0.0

        # Entrenar
        logger.info(f"  Training on {len(train_inner_df)} obs...")
        nf = NeuralForecast(models=[model], freq='D')

        try:
            nf.fit(df=train_inner_df)
            logger.info(f"  ‚úì Training completed")
        except Exception as e:
            logger.error(f"  ‚úó Training failed: {e}")
            return 0.0

        # Predicciones rolling
        logger.info(f"  Starting rolling predictions on {len(valid_df)} obs...")

        all_predictions = []
        all_true_values = []

        # ‚ö° LOG cada 20 predicciones para monitorear progreso
        for i in range(len(valid_df)):
            if i % 20 == 0:
                logger.info(f"    Prediction {i+1}/{len(valid_df)} ({i/len(valid_df)*100:.1f}%)")

            try:
                # Historia
                if i == 0:
                    hist_df = train_inner_df.copy()
                else:
                    hist_df = pd.concat([
                        train_inner_df,
                        valid_df.iloc[:i]
                    ], ignore_index=True)

                # Predecir
                forecast = nf.predict(df=hist_df)

                # Extraer √∫ltimo valor
                pred = forecast['NBEATSx'].values[-1]
                true_val = valid_df.iloc[i]['y']

                all_predictions.append(pred)
                all_true_values.append(true_val)

            except Exception as e:
                logger.error(f"    Prediction {i} failed: {e}")
                # Si falla una predicci√≥n, continuar con las dem√°s
                all_predictions.append(np.nan)
                all_true_values.append(true_val)

        logger.info(f"  ‚úì Completed {len(all_predictions)} predictions")

        # Remover NaNs si los hay
        all_predictions = np.array(all_predictions)
        all_true_values = np.array(all_true_values)

        valid_mask = ~np.isnan(all_predictions)
        y_pred_levels = all_predictions[valid_mask]
        y_true_levels = all_true_values[valid_mask]

        logger.info(f"  Valid predictions: {len(y_pred_levels)}/{len(all_predictions)}")

        if len(y_pred_levels) < 10:
            logger.error(f"  ‚úó Too few valid predictions")
            return 0.0

        # Calcular DA
        logger.info(f"  Calculating DA...")
        da = self._calculate_da_from_predictions(y_true_levels, y_pred_levels)

        # SANITY CHECK
        if da > 70:
            logger.warning(f"  üö® DA={da:.2f}% TOO HIGH - Penalizing")
            return 0.0

        if da < 30:
            logger.warning(f"  ‚ö†Ô∏è DA={da:.2f}% TOO LOW - Penalizing")
            return 0.0

        logger.info(f"  ‚úì Trial completed: DA={da:.2f}%")
        logger.info(f"{'='*60}\n")

        return da

    def tune(self, train_df_long, hist_exog_list, n_trials, timeout):
        """
        Tuning para h=1 con valid set REDUCIDO
        """
        logger.info("="*80)
        logger.info(f"HYPERPARAMETER TUNING h={self.h} - DEBUG MODE")
        logger.info("="*80)

        # Split train ‚Üí train_inner (80%) + valid_full (20%)
        total_len = len(train_df_long)
        split_idx = int(total_len * 0.8)

        train_inner_df = train_df_long.iloc[:split_idx].copy()
        valid_full_df = train_df_long.iloc[split_idx:].copy()

        # ‚ö° CR√çTICO: Reducir valid a 100 obs para debug r√°pido
        n_valid_tuning = min(100, len(valid_full_df))
        valid_df = valid_full_df.iloc[-n_valid_tuning:].copy()

        logger.info(f"\nData Split:")
        logger.info(f"  Total train: {total_len} obs")
        logger.info(f"  Train_inner: {len(train_inner_df)} obs (80%)")
        logger.info(f"  Validation (full): {len(valid_full_df)} obs (20%)")
        logger.info(f"  Validation (tuning): {len(valid_df)} obs ‚ö° REDUCED FOR SPEED")
        logger.info(f"\nOptuna Configuration:")
        logger.info(f"  N trials: {n_trials}")
        logger.info(f"  Timeout: {timeout}s")
        logger.info(f"  Hist exog: {len(hist_exog_list)} variables")
        logger.info("")

        # Optuna study
        study = optuna.create_study(
            direction='maximize',
            sampler=TPESampler(seed=RANDOM_STATE)
        )

        # Optimize
        try:
            study.optimize(
                lambda trial: self._objective(trial, train_inner_df, valid_df,
                                             train_df_long, hist_exog_list),
                n_trials=n_trials,
                timeout=timeout,
                show_progress_bar=True
            )
        except KeyboardInterrupt:
            logger.warning("\n‚ö†Ô∏è Optimization interrupted by user")
            logger.info(f"Completed {len(study.trials)} trials before interruption")

        # Verificar trials v√°lidos
        valid_trials = [t for t in study.trials if t.value is not None and t.value > 0]

        if len(valid_trials) == 0:
            logger.error("‚ùå NO VALID TRIALS")
            raise ValueError("No valid trials found")

        # Extract best config
        best_params = study.best_params
        stack_config = eval(best_params['stack_config'])

        best_config = {
            'h': self.h,
            'input_size': best_params['input_size'],
            'max_steps': best_params['max_steps'],
            'learning_rate': best_params['learning_rate'],
            'stack_types': stack_config[0],
            'n_blocks': stack_config[1],
            'mlp_units_size': best_params['mlp_units_size'],
            'batch_size': best_params['batch_size']
        }

        best_metrics = {
            'DA_valid': study.best_value,
            'MASE_valid': None
        }

        logger.info(f"\n{'='*80}")
        logger.info(f"TUNING RESULTS:")
        logger.info(f"  Valid trials: {len(valid_trials)}/{len(study.trials)}")
        logger.info(f"  Best DA: {study.best_value:.2f}%")

        if study.best_value > 70:
            logger.error(f"  üö® DA TOO HIGH - Check for leakage")
        elif study.best_value < 45:
            logger.warning(f"  ‚ö†Ô∏è DA is low")
        else:
            logger.info(f"  ‚úì DA in expected range")

        logger.info(f"\nBest config:")
        for key, val in best_config.items():
            logger.info(f"  {key}: {val}")

        logger.info(f"\n{'='*80}")

        return {
            'best_config': best_config,
            'best_metrics': best_metrics,
            'study': study,
            'n_valid_trials': len(valid_trials)
        }

logger.info("NBEATSxTuner loaded (DEBUG MODE)")
print("‚úì Celda 7: NBEATSxTuner (DEBUG - 100 obs validation)")

# ============================================================================
# CELDA 8: ExogenousSelector (h=1 - CORRECTED v2)
# ============================================================================

class ExogenousSelector:
    """
    Selector de ex√≥genas con h=1 SIN LEAKAGE
    ‚úÖ Usa hist_exog_list (NO futr_exog_list)
    """

    def __init__(self, h: int = H_FORECAST):
        self.builder = NBEATSxBuilder()
        self.tuner = NBEATSxTuner(h=h)
        self.h = h

    def select(self, train_df_long, train_df_wide, exog_candidates, best_config):
        """Selecci√≥n exhaustiva de ex√≥genas para h=1"""
        logger.info("="*80)
        logger.info(f"EXOGENOUS SELECTION h={self.h} - SIN LEAKAGE")
        logger.info("="*80)

        # Split train ‚Üí train_inner + valid (igual que tuning)
        total_len = len(train_df_long)
        split_idx = int(total_len * 0.8)

        train_inner_df = train_df_long.iloc[:split_idx].copy()
        valid_full_df = train_df_long.iloc[split_idx:].copy()

        # Usar valid reducido (100 obs) para velocidad
        n_valid_tuning = min(100, len(valid_full_df))
        valid_df = valid_full_df.iloc[-n_valid_tuning:].copy()

        logger.info(f"Candidatas: {len(exog_candidates)}")
        logger.info(f"Train_inner: {len(train_inner_df)} | Valid: {len(valid_df)} obs")

        # Preparar train returns para MASE
        train_returns = np.log(train_df_wide['PEN_target'] / train_df_wide['PEN_lag_1'])
        train_returns = train_returns.dropna().values

        all_results = []

        for i, exog in enumerate(exog_candidates):
            logger.info(f"\n[{i+1}/{len(exog_candidates)}] Evaluando: {exog}")

            # ‚úÖ CR√çTICO: hist_exog_list (NO futr_exog_list)
            if exog == 'NONE':
                hist_exog_list = []
            else:
                hist_exog_list = [exog]

            # Build model
            try:
                model = self.builder.build_model(
                    h=self.h,
                    input_size=best_config['input_size'],
                    max_steps=best_config['max_steps'],
                    learning_rate=best_config['learning_rate'],
                    stack_types=best_config['stack_types'],
                    n_blocks=best_config['n_blocks'],
                    mlp_units_size=best_config['mlp_units_size'],
                    batch_size=best_config['batch_size'],
                    hist_exog_list=hist_exog_list,  # ‚úÖ CAMBIO AQU√ç
                    val_check_steps=50,
                    early_stop_patience_steps=-1
                )
            except Exception as e:
                logger.warning(f"   Model build failed: {e}")
                all_results.append({'exog': exog, 'DA': 0.0, 'MASE': np.inf, 'MAE': np.inf})
                continue

            # Train en train_inner
            nf = NeuralForecast(models=[model], freq='D')

            try:
                logger.info(f"   Training...")
                nf.fit(df=train_inner_df)
                logger.info(f"   ‚úì Trained")

                # Predicciones rolling
                logger.info(f"   Predicting...")
                all_predictions = []
                all_true_values = []

                for j in range(len(valid_df)):
                    if j == 0:
                        hist_df = train_inner_df.copy()
                    else:
                        hist_df = pd.concat([train_inner_df, valid_df.iloc[:j]], ignore_index=True)

                    forecast = nf.predict(df=hist_df)
                    all_predictions.append(forecast['NBEATSx'].values[-1])
                    all_true_values.append(valid_df.iloc[j]['y'])

                y_pred_levels = np.array(all_predictions)
                y_true_levels = np.array(all_true_values)

                # Calcular DA
                da = self.tuner._calculate_da_from_predictions(y_true_levels, y_pred_levels)

                # Calcular MASE
                y_true_ret = np.diff(np.log(y_true_levels))
                y_pred_ret = np.diff(np.log(y_pred_levels))
                mae = np.mean(np.abs(y_true_ret - y_pred_ret))
                mae_naive = np.mean(np.abs(np.diff(train_returns)))
                mase = mae / mae_naive if mae_naive > 0 else np.inf

                all_results.append({'exog': exog, 'DA': da, 'MASE': mase, 'MAE': mae})
                logger.info(f"   ‚úì DA: {da:.2f}% | MASE: {mase:.4f}")

            except Exception as e:
                logger.warning(f"   Evaluation failed: {e}")
                all_results.append({'exog': exog, 'DA': 0.0, 'MASE': np.inf, 'MAE': np.inf})

        # Seleccionar mejor
        all_results_sorted = sorted(all_results, key=lambda x: (-x['DA'], x['MASE']))
        best_result = all_results_sorted[0]

        logger.info(f"\n{'='*80}")
        logger.info("TOP 5 EXOGENOUS:")
        for i, res in enumerate(all_results_sorted[:5]):
            logger.info(f"{i+1}. {res['exog']}: DA={res['DA']:.2f}%, MASE={res['MASE']:.4f}")

        logger.info(f"\nBEST: {best_result['exog']} (DA={best_result['DA']:.2f}%)")
        logger.info("="*80)

        return {
            'best_exog': best_result['exog'],
            'best_metrics': {'DA': best_result['DA'], 'MASE': best_result['MASE']},
            'all_results': all_results
        }

logger.info("ExogenousSelector loaded (hist_exog_list)")
print("‚úì Celda 8: ExogenousSelector (CORRECTED - hist_exog_list)")

# ============================================================================
# CELDA 9: FinalEvaluator (h=1 - CORRECTED v2)
# ============================================================================

class FinalEvaluator:
    """Evaluaci√≥n final en holdout SIN LEAKAGE"""

    def __init__(self, h: int = H_FORECAST):
        self.builder = NBEATSxBuilder()
        self.h = h

    def train_and_evaluate(self, train_df_long, holdout_df_long, full_df,
                          best_config, best_exog, train_returns):
        """Entrenamiento final y evaluaci√≥n para h=1"""
        logger.info("="*80)
        logger.info(f"FINAL EVALUATION h={self.h} - SIN LEAKAGE")
        logger.info("="*80)

        # ‚úÖ CR√çTICO: hist_exog_list (NO futr_exog_list)
        if best_exog == 'NONE':
            hist_exog_list = []
        else:
            hist_exog_list = [best_exog]

        logger.info(f"Best exog: {best_exog}")
        logger.info(f"Training: {len(train_df_long)} obs")
        logger.info(f"Holdout: {len(holdout_df_long)} obs")

        # Build model
        model = self.builder.build_model(
            h=self.h,
            input_size=best_config['input_size'],
            max_steps=best_config['max_steps'],
            learning_rate=best_config['learning_rate'],
            stack_types=best_config['stack_types'],
            n_blocks=best_config['n_blocks'],
            mlp_units_size=best_config['mlp_units_size'],
            batch_size=best_config['batch_size'],
            hist_exog_list=hist_exog_list,  # ‚úÖ CAMBIO AQU√ç
            val_check_steps=50,
            early_stop_patience_steps=-1
        )

        # Train SOLO en train
        nf = NeuralForecast(models=[model], freq='D')
        nf.fit(df=train_df_long)
        logger.info("‚úì Model trained (NO LEAKAGE)")

        # Predicciones rolling en holdout
        logger.info("\nüìä PREDICIENDO HOLDOUT:")
        all_predictions = []
        all_true_values = []
        all_dates = []

        for i in range(len(holdout_df_long)):
            if i == 0:
                hist_df = train_df_long.copy()
            else:
                hist_df = pd.concat([train_df_long, holdout_df_long.iloc[:i]], ignore_index=True)

            try:
                forecast = nf.predict(df=hist_df)
                pred = forecast['NBEATSx'].values[-1]
                true_val = holdout_df_long.iloc[i]['y']
                date = holdout_df_long.iloc[i]['ds']

                all_predictions.append(pred)
                all_true_values.append(true_val)
                all_dates.append(date)

                if (i+1) % 10 == 0:
                    logger.info(f"   {i+1}/{len(holdout_df_long)}")

            except Exception as e:
                logger.error(f"   Error {i+1}: {e}")
                pred = all_true_values[-1] if i > 0 else train_df_long['y'].iloc[-1]
                all_predictions.append(pred)
                all_true_values.append(holdout_df_long.iloc[i]['y'])
                all_dates.append(holdout_df_long.iloc[i]['ds'])

        y_pred_level = np.array(all_predictions)
        y_true_level = np.array(all_true_values)
        dates = np.array(all_dates)

        logger.info(f"\n‚úì {len(y_pred_level)} predictions generated")

        # Calcular y_prev_level
        y_prev_level = []
        for date in dates:
            idx = full_df.index.get_loc(pd.Timestamp(date))
            prev_val = full_df['PEN_target'].iloc[idx - 1] if idx > 0 else full_df['PEN_target'].iloc[idx]
            y_prev_level.append(prev_val)
        y_prev_level = np.array(y_prev_level)

        # Calcular m√©tricas
        y_true_ret = np.log(y_true_level / y_prev_level)
        y_pred_ret = np.log(y_pred_level / y_prev_level)

        dir_true = np.sign(y_true_ret)
        dir_pred = np.sign(y_pred_ret)

        correct = np.sum(dir_true == dir_pred)
        da = 100.0 * correct / len(dir_true)

        mae_ret = np.mean(np.abs(y_true_ret - y_pred_ret))
        mae_naive = np.mean(np.abs(np.diff(train_returns)))
        mase = mae_ret / mae_naive if mae_naive > 0 else np.inf
        mae_level = np.mean(np.abs(y_true_level - y_pred_level))

        metrics = {
            'DA': da,
            'MASE': mase,
            'MAE_returns': mae_ret,
            'MAE_levels': mae_level
        }

        comparison = FXMetrics.compare_to_baseline(metrics)

        logger.info(f"\nüìä M√âTRICAS HOLDOUT:")
        logger.info(f"  DA: {da:.2f}%")
        logger.info(f"  MASE: {mase:.4f}")
        logger.info(f"\nüéØ VERDICT: {comparison['verdict']}")
        logger.info("="*80)

        return {
            'best_config': best_config,
            'best_exog': best_exog,
            'holdout_metrics': metrics,
            'comparison': comparison,
            'predictions': {
                'y_true_level': y_true_level,
                'y_pred_level': y_pred_level,
                'y_prev_level': y_prev_level,
                'dates': dates
            },
            'model': nf
        }

logger.info("FinalEvaluator loaded (hist_exog_list)")
print("‚úì Celda 9: FinalEvaluator (CORRECTED - hist_exog_list)")

# MODIFICACI√ìN 4: CELDA 9.5 (NUEVA) - Funci√≥n generate_oof_predictions
# ============================================================================
# CREAR NUEVA CELDA despu√©s de CELDA 9 (FinalEvaluator) y antes de CELDA 10
# COPIAR TODO ESTE C√ìDIGO:
# ----------------------------------------------------------------------------
import pickle
from pathlib import Path

def generate_oof_predictions_nbeatsx(train_df_long_scaled, train_df_wide,
                                      best_config, best_exog, train_returns,
                                      checkpoint_dir=None):
    """
    Generar predicciones Out-of-Fold usando Walk-Forward Cross-Validation.
    CON CHECKPOINTING INCREMENTAL.

    Args:
        train_df_long_scaled: DataFrame en formato long (scaled)
        train_df_wide: DataFrame en formato wide (para obtener PEN_lag_1)
        best_config: dict con hiperpar√°metros √≥ptimos
        best_exog: mejor ex√≥gena seleccionada
        train_returns: array de log returns del train
        checkpoint_dir: directorio para guardar checkpoints (default: OOF_DIR)

    Returns:
        dict con predicciones OOF, m√©tricas y metadata
    """

    if checkpoint_dir is None:
        checkpoint_dir = OOF_DIR

    checkpoint_dir = Path(checkpoint_dir)
    checkpoint_dir.mkdir(parents=True, exist_ok=True)

    # Archivo de checkpoint temporal
    checkpoint_file = checkpoint_dir / 'oof_nbeatsx_checkpoint.pkl'

    print("\n" + "="*80)
    print("üîÑ GENERANDO OOF PREDICTIONS - WALK-FORWARD CV CON CHECKPOINTS")
    print("="*80)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PASO 1: Verificar si existe checkpoint
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    resume_from_checkpoint = False
    checkpoint_data = None

    if checkpoint_file.exists():
        print(f"\nüìÇ CHECKPOINT ENCONTRADO: {checkpoint_file}")
        print("   ¬øQuieres continuar desde el checkpoint o empezar de cero?")
        print("   1. CONTINUAR desde checkpoint (recomendado)")
        print("   2. EMPEZAR DE CERO (borra progreso)")

        # Para automatizar, asumimos que quiere continuar
        # Si quiere empezar de cero, debe eliminar manualmente el checkpoint
        use_checkpoint = True  # Cambiar a False para forzar reinicio

        if use_checkpoint:
            try:
                with open(checkpoint_file, 'rb') as f:
                    checkpoint_data = pickle.load(f)

                print(f"\n‚úÖ Checkpoint cargado:")
                print(f"   √öltimo fold completado: {checkpoint_data['last_fold']}")
                print(f"   Predicciones guardadas: {len(checkpoint_data['oof_predictions'])}")
                print(f"   Tiempo transcurrido: {checkpoint_data['elapsed_minutes']:.1f} min")

                resume_from_checkpoint = True

            except Exception as e:
                print(f"\n‚ö†Ô∏è Error cargando checkpoint: {e}")
                print("   Empezando desde cero...")
                resume_from_checkpoint = False
        else:
            print("\nüîÑ Empezando de cero (borrando checkpoint)...")
            checkpoint_file.unlink()
            resume_from_checkpoint = False

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PASO 2: Inicializar o recuperar estado
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    if resume_from_checkpoint and checkpoint_data:
        # Recuperar estado
        oof_predictions = checkpoint_data['oof_predictions']
        oof_dates = checkpoint_data['oof_dates']
        oof_actuals = checkpoint_data['oof_actuals']
        fold_metrics = checkpoint_data['fold_metrics']
        start_fold = checkpoint_data['last_fold'] + 1
        cumulative_time = checkpoint_data['elapsed_seconds']

        print(f"\n‚ñ∂Ô∏è RESUMIENDO desde fold {start_fold}")

    else:
        # Estado nuevo
        oof_predictions = []
        oof_dates = []
        oof_actuals = []
        fold_metrics = []
        start_fold = 1
        cumulative_time = 0

        print(f"\n‚ñ∂Ô∏è INICIANDO desde fold 1")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PASO 3: Configuraci√≥n Walk-Forward
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    builder = NBEATSxBuilder()

    # Preparar hist_exog_list
    if best_exog == 'NONE':
        hist_exog_list = []
    else:
        hist_exog_list = [best_exog]

    # Par√°metros Walk-Forward
    n_obs = len(train_df_long_scaled)
    n_splits = (n_obs - MIN_TRAIN) // STEP_SIZE

    print(f"\nüìä Configuraci√≥n Walk-Forward:")
    print(f"   Total observaciones train: {n_obs}")
    print(f"   M√≠nimo entrenamiento: {MIN_TRAIN}")
    print(f"   Step size: {STEP_SIZE}")
    print(f"   Splits totales: {n_splits}")
    print(f"   Best exog: {best_exog}")

    start_time = datetime.now()

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PASO 4: Walk-Forward Loop con Checkpointing
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    fold = 0
    positions = list(range(MIN_TRAIN, n_obs, STEP_SIZE))

    for start in positions:
        fold += 1

        # Si estamos resumiendo, saltar folds ya completados
        if fold < start_fold:
            continue

        end = min(start + STEP_SIZE, n_obs)

        # Split
        train_fold = train_df_long_scaled.iloc[:start].copy()
        val_fold = train_df_long_scaled.iloc[start:end].copy()

        if len(val_fold) == 0:
            continue

        print(f"\n{'‚îÄ'*60}")
        print(f"Fold {fold}/{n_splits} [{fold-start_fold+1}/{n_splits-start_fold+1} desde checkpoint]")
        print(f"   Train: 0 ‚Üí {start} ({start} obs)")
        print(f"   Val:   {start} ‚Üí {end} ({len(val_fold)} obs)")

        try:
            # Build model con configuraci√≥n √≥ptima (max_steps reducido para OOF)
            model = builder.build_model(
                h=H_FORECAST,
                input_size=best_config['input_size'],
                max_steps=min(best_config['max_steps'], 200),  # Reducido para velocidad
                learning_rate=best_config['learning_rate'],
                stack_types=best_config['stack_types'],
                n_blocks=best_config['n_blocks'],
                mlp_units_size=best_config['mlp_units_size'],
                batch_size=best_config['batch_size'],
                hist_exog_list=hist_exog_list,
                val_check_steps=50,
                early_stop_patience_steps=-1
            )

            # Entrenar en train_fold
            nf = NeuralForecast(models=[model], freq='D')
            nf.fit(df=train_fold)

            # Predicciones rolling en val_fold
            fold_preds = []
            fold_true = []
            fold_dates_list = []

            for j in range(len(val_fold)):
                if j == 0:
                    hist_df = train_fold.copy()
                else:
                    hist_df = pd.concat([train_fold, val_fold.iloc[:j]], ignore_index=True)

                try:
                    forecast = nf.predict(df=hist_df)
                    pred = forecast['NBEATSx'].values[-1]
                except:
                    pred = hist_df['y'].iloc[-1]  # Fallback

                fold_preds.append(pred)
                fold_true.append(val_fold.iloc[j]['y'])
                fold_dates_list.append(val_fold.iloc[j]['ds'])

            fold_preds = np.array(fold_preds)
            fold_true = np.array(fold_true)

            # Obtener y_prev para calcular returns
            val_indices = list(range(start, end))
            y_prev_fold = []
            for idx in val_indices:
                if idx > 0:
                    y_prev_fold.append(train_df_long_scaled.iloc[idx-1]['y'])
                else:
                    y_prev_fold.append(train_df_long_scaled.iloc[0]['y'])
            y_prev_fold = np.array(y_prev_fold)[:len(fold_preds)]

            # Calcular log returns
            fold_true_ret = np.log(fold_true / y_prev_fold)
            fold_pred_ret = np.log(fold_preds / y_prev_fold)

            # M√©tricas del fold
            fold_da = FXMetrics.directional_accuracy(fold_true_ret, fold_pred_ret)
            fold_mae = np.mean(np.abs(fold_true_ret - fold_pred_ret))

            print(f"   ‚úÖ DA: {fold_da:.2f}% | MAE: {fold_mae:.6f}")

            # Guardar predicciones como LOG RETURNS
            oof_predictions.extend(fold_pred_ret.tolist())
            oof_dates.extend(fold_dates_list)
            oof_actuals.extend(fold_true_ret.tolist())

            fold_metrics.append({
                'fold': fold,
                'train_size': start,
                'val_size': len(val_fold),
                'da': float(fold_da),
                'mae': float(fold_mae)
            })

        except Exception as e:
            print(f"   ‚ö†Ô∏è Error en fold {fold}: {e}")
            # Rellenar con zeros para mantener alineaci√≥n
            for j in range(len(val_fold)):
                oof_predictions.append(0.0)
                oof_dates.append(val_fold.iloc[j]['ds'])
                oof_actuals.append(0.0)

            fold_metrics.append({
                'fold': fold,
                'train_size': start,
                'val_size': len(val_fold),
                'da': 0.0,
                'mae': np.inf,
                'error': str(e)
            })

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # GUARDAR CHECKPOINT CADA 5 FOLDS
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

        if fold % 5 == 0 or fold == n_splits:
            elapsed_now = (datetime.now() - start_time).total_seconds()
            total_elapsed = cumulative_time + elapsed_now

            checkpoint_data_save = {
                'last_fold': fold,
                'oof_predictions': oof_predictions.copy(),
                'oof_dates': oof_dates.copy(),
                'oof_actuals': oof_actuals.copy(),
                'fold_metrics': fold_metrics.copy(),
                'elapsed_seconds': total_elapsed,
                'elapsed_minutes': total_elapsed / 60,
                'timestamp': datetime.now().isoformat(),
                'best_config': best_config,
                'best_exog': best_exog
            }

            with open(checkpoint_file, 'wb') as f:
                pickle.dump(checkpoint_data_save, f)

            print(f"\nüíæ CHECKPOINT GUARDADO (fold {fold}/{n_splits})")
            print(f"   Archivo: {checkpoint_file}")
            print(f"   Tiempo acumulado: {total_elapsed/60:.1f} min")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PASO 5: Calcular m√©tricas finales y guardar resultado definitivo
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    # Convertir a arrays
    oof_predictions = np.array(oof_predictions)
    oof_actuals = np.array(oof_actuals)

    elapsed_total = cumulative_time + (datetime.now() - start_time).total_seconds()

    # M√©tricas globales OOF
    valid_mask = (oof_predictions != 0) & (oof_actuals != 0)
    if valid_mask.sum() > 0:
        valid_preds = oof_predictions[valid_mask]
        valid_actuals = oof_actuals[valid_mask]
        da_oof = FXMetrics.directional_accuracy(valid_actuals, valid_preds)
        mae_oof = np.mean(np.abs(valid_actuals - valid_preds))
        mae_naive = np.mean(np.abs(np.diff(train_returns)))
        mase_oof = mae_oof / mae_naive if mae_naive > 0 else np.inf
    else:
        da_oof = 0.0
        mae_oof = 0.0
        mase_oof = 0.0

    n_zeros = np.sum(oof_predictions == 0)

    print(f"\n{'='*60}")
    print(f"üìä RESUMEN OOF FINAL")
    print(f"{'='*60}")
    print(f"   Observaciones: {len(oof_predictions)}")
    print(f"   Zeros:         {n_zeros} {'‚úÖ' if n_zeros == 0 else '‚ö†Ô∏è'}")
    print(f"   DA OOF:        {da_oof:.2f}%")
    print(f"   MAE OOF:       {mae_oof:.6f}")
    print(f"   MASE OOF:      {mase_oof:.4f}")
    print(f"   Tiempo total:  {elapsed_total/60:.1f} minutos")

    # Guardar OOF definitivo
    save_oof_predictions(
        predictions=oof_predictions,
        dates=oof_dates,
        actuals=oof_actuals,
        model_name='NBEATSx',
        prediction_type='log_returns',
        metadata={
            'method': 'walk_forward_with_checkpoints',
            'min_train': MIN_TRAIN,
            'step_size': STEP_SIZE,
            'n_splits': len(fold_metrics),
            'n_observations': len(oof_predictions),
            'n_zeros': int(n_zeros),
            'model': 'NBEATSx',
            'best_exog': best_exog,
            'hyperparameters': best_config,
            'da_oof': float(da_oof),
            'mae_oof': float(mae_oof),
            'mase_oof': float(mase_oof),
            'elapsed_minutes': float(elapsed_total/60),
            'fold_metrics': fold_metrics,
            'resumed_from_checkpoint': resume_from_checkpoint,
            'checkpoint_frequency': 5
        },
        output_dir=OOF_DIR
    )

    # Eliminar checkpoint temporal ahora que terminamos
    if checkpoint_file.exists():
        checkpoint_file.unlink()
        print(f"\nüóëÔ∏è Checkpoint temporal eliminado (proceso completado)")

    print(f"\n‚úÖ OOF completado exitosamente")
    print(f"   üìÅ Guardado en: {OOF_DIR}/train_oof_NBEATSx.csv")

    return {
        'predictions': oof_predictions,
        'dates': oof_dates,
        'actuals': oof_actuals,
        'metrics': {
            'da': da_oof,
            'mae': mae_oof,
            'mase': mase_oof
        },
        'fold_metrics': fold_metrics
    }


logger.info("generate_oof_predictions_nbeatsx loaded (WITH CHECKPOINTING)")
print("‚úì Celda 9.5 completada: generate_oof_predictions_nbeatsx con checkpoints cargada")

# ============================================================================
# CELDA 10: ModelPersistence (h=1 VERSION - CORRECTED)
# ============================================================================

import pickle

class ModelPersistence:
    """
    Persistencia de modelo y metadata para h=1
    """

    @staticmethod
    def save_model(nf, filename):
        """Guarda modelo NeuralForecast"""
        filepath = OUTPUT_DIR / filename
        with open(filepath, 'wb') as f:
            pickle.dump(nf, f)
        logger.info(f"‚úì Model saved: {filepath}")
        return filepath

    @staticmethod
    def save_metrics(final_result, tuning_result, selection_result):
        """Guarda m√©tricas completas en JSON"""

        # ‚úÖ CORRECCI√ìN: Usar 'DA_valid' (no 'DA_cv')
        metrics = {
            'run_id': datetime.now().strftime("%Y%m%d_%H%M%S"),
            'model': 'NBEATSx',
            'framework': 'NeuralForecast',
            'horizon': 'h=1 (ONE-STEP-AHEAD)',

            'best_config': final_result['best_config'],
            'best_exog': final_result['best_exog'],

            'holdout_metrics': final_result['holdout_metrics'],

            'comparison': {
                'baseline_arx': final_result['comparison']['baseline_arx'],
                'baseline_nhits': final_result['comparison']['baseline_nhits'],
                'nbeatsx': final_result['comparison']['nbeatsx'],
                'delta_vs_arx': final_result['comparison']['delta_vs_arx'],
                'delta_vs_nhits': final_result['comparison']['delta_vs_nhits'],
                'verdict': final_result['comparison']['verdict']
            },

            'tuning': {
                'n_trials': N_TRIALS,
                'best_da_validation': tuning_result['best_metrics']['DA_valid'],  # ‚úÖ CAMBIO AQU√ç
                'n_valid_trials': tuning_result['n_valid_trials']
            },

            'selection': {
                'n_candidates': len(selection_result['all_results']),
                'best_exog': selection_result['best_exog'],
                'best_da_validation': selection_result['best_metrics']['DA'],  # ‚úÖ AGREGADO
                'all_results': selection_result['all_results']
            },

            'metadata': {
                'train_size': None,  # Set in main execution
                'holdout_size': N_HOLDOUT,
                'min_train': MIN_TRAIN,
                'step_size': STEP_SIZE,
                'random_state': RANDOM_STATE,
                'gpu_used': torch.cuda.is_available(),
                'evaluation_method': 'rolling_forecast_manual (NO LEAKAGE)',
                'tuning_method': 'train_valid_split_80_20 (NO CV refitting)'
            },

            'timestamp': datetime.now().isoformat()
        }

        filepath = OUTPUT_DIR / 'metrics.json'
        with open(filepath, 'w') as f:
            json.dump(metrics, f, indent=2)

        logger.info(f"‚úì Metrics saved: {filepath}")
        return filepath

    @staticmethod
    def save_config(best_config, best_exog):
        """Guarda configuraci√≥n final"""
        config = {
            'horizon': 'h=1',
            'best_config': best_config,
            'best_exog': best_exog,
            'timestamp': datetime.now().isoformat()
        }

        filepath = OUTPUT_DIR / 'config.json'
        with open(filepath, 'w') as f:
            json.dump(config, f, indent=2)

        logger.info(f"‚úì Config saved: {filepath}")
        return filepath

    @staticmethod
    def create_readme(final_result):
        """Crea README con interpretaci√≥n para h=1"""
        comparison = final_result['comparison']
        metrics = final_result['holdout_metrics']
        best_exog = final_result['best_exog']

        # Determinar interpretaci√≥n de best_exog
        if best_exog == 'NONE':
            exog_interpretation = """### 1. Best Exogenous: NONE
Confirma hallazgo de NHITS: Ex√≥genas NO ayudan.
USD/PEN es un mercado eficiente donde variables externas no mejoran predicci√≥n.
Problema es fundamental (EMH), NO arquitectural."""
        else:
            exog_interpretation = f"""### 1. Best Exogenous: {best_exog}
NBEATSx S√ç aprovecha ex√≥genas t√©cnicas ({best_exog}).
La arquitectura basis expansion captura patrones no-lineales que ARX/NHITS no detectan."""

        readme = f"""# NBEATSx h=1 Experiment - USD/PEN Forecasting

## Configuration
- **Model**: NBEATSx (Basis Expansion)
- **Horizon**: h=1 (ONE-STEP-AHEAD)
- **Input Size**: {final_result['best_config']['input_size']} d√≠as
- **Stack Types**: {final_result['best_config']['stack_types']} (ONLY identity for h=1)
- **Best Exogenous**: {best_exog}

## Results

### Holdout Metrics ({N_HOLDOUT} d√≠as - rolling forecast manual)
- **DA**: {metrics['DA']:.2f}%
- **MASE**: {metrics['MASE']:.4f}
- **MAE (returns)**: {metrics['MAE_returns']:.6f}
- **MAE (levels)**: {metrics['MAE_levels']:.4f}

### Comparison vs Baselines (TODOS h=1)
| Modelo | DA | MASE | Arquitectura |
|--------|-------|------|--------------|
| ARX | {comparison['baseline_arx']['DA']:.2f}% | {comparison['baseline_arx']['MASE']:.4f} | Lineal cl√°sico |
| NHITS | {comparison['baseline_nhits']['DA']:.2f}% | {comparison['baseline_nhits']['MASE']:.4f} | Pooling jer√°rquico |
| **NBEATSx** | **{metrics['DA']:.2f}%** | **{metrics['MASE']:.4f}** | **Basis expansion** |

**Œî vs ARX**: {comparison['delta_vs_arx']['DA']:+.2f}% DA, {comparison['delta_vs_arx']['MASE']:+.4f} MASE
**Œî vs NHITS**: {comparison['delta_vs_nhits']['DA']:+.2f}% DA, {comparison['delta_vs_nhits']['MASE']:+.4f} MASE

## Verdict
{comparison['verdict']}

## Key Findings

{exog_interpretation}

### 2. Comparaci√≥n Justa (h=1 vs h=1)
Esta implementaci√≥n permite comparaci√≥n directa con ARX y NHITS:
- **Mismo horizonte** (h=1)
- **Mismas m√©tricas** (DA, MASE)
- **Mismo holdout** ({N_HOLDOUT} d√≠as)

### 3. Limitaci√≥n h=1
Con horizonte h=1, NBEATSx solo puede usar **identity stacks**:
- Trend stack: NO disponible (requiere h>1)
- Seasonality stack: NO disponible (requiere h>1)
- Identity stack: √önico disponible (generic basis expansion)

### 4. Evaluaci√≥n Sin Leakage ‚úÖ
**CR√çTICO**: Esta implementaci√≥n usa metodolog√≠a rigurosa:
- Train-validation split fijo (80-20%) para tuning
- Train SOLO en train set para modelo final
- Rolling forecast manual en holdout (sin refitting)
- hist_exog_list (variables hist√≥ricas lag_1 solamente)
- NO cross_validation con refitting

## Methodology
- **Hyperparameter Tuning**: Train-validation split (80-20%), {N_TRIALS} trials Optuna
- **Exogenous Selection**: Exhaustive evaluation (1 variable a la vez)
- **Holdout Evaluation**: Rolling forecast manual (NO LEAKAGE)
- **Random State**: {RANDOM_STATE} (reproducibilidad)

## Files
- `nbeatsx_h1_final.pkl`: Trained model
- `metrics.json`: Complete metrics
- `config.json`: Best configuration
- `checkpoints/`: Intermediate results (tuning, selection, dataframes)
- `*.png`: Visualizations

---
*Generated: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}*
*Corrected implementation - Zero data leakage - hist_exog_list only*
"""

        filepath = OUTPUT_DIR / 'README.md'
        with open(filepath, 'w') as f:
            f.write(readme)

        logger.info(f"‚úì README saved: {filepath}")
        return filepath

logger.info("ModelPersistence class loaded successfully (h=1 version - CORRECTED)")
print("‚úì Celda 10 completada: ModelPersistence h=1 (DA_valid compatible)")

# ============================================================================
# CELDA 10.5: CheckpointManager - Sistema de Persistencia
# ============================================================================

import pickle
from pathlib import Path

class CheckpointManager:
    """
    Sistema de checkpointing para reanudar experimentos

    Guarda:
    - tuning_result (objeto completo con estudio Optuna)
    - selection_result (resultados de todas las ex√≥genas)
    - DataFrames procesados (train/holdout escalados)

    Uso:
    1. Despu√©s de tuning: checkpoint.save('tuning', tuning_result)
    2. Despu√©s de selection: checkpoint.save('selection', selection_result)
    3. Al reiniciar: result = checkpoint.load('tuning')
    """

    def __init__(self, output_dir: Path = OUTPUT_DIR):
        self.output_dir = Path(output_dir)
        self.checkpoint_dir = self.output_dir / 'checkpoints'
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)

        logger.info(f"CheckpointManager initialized: {self.checkpoint_dir}")

    def save(self, name: str, obj: any, force: bool = False):
        """
        Guarda objeto en checkpoint

        Args:
            name: Nombre del checkpoint ('tuning', 'selection', 'dataframes')
            obj: Objeto a guardar
            force: Si True, sobrescribe checkpoint existente
        """
        filepath = self.checkpoint_dir / f"{name}.pkl"

        if filepath.exists() and not force:
            logger.warning(f"Checkpoint '{name}' already exists. Use force=True to overwrite.")
            return filepath

        try:
            with open(filepath, 'wb') as f:
                pickle.dump(obj, f, protocol=pickle.HIGHEST_PROTOCOL)

            size_mb = filepath.stat().st_size / 1024 / 1024
            logger.info(f"‚úì Checkpoint saved: {name} ({size_mb:.2f} MB)")
            return filepath

        except Exception as e:
            logger.error(f"‚úó Failed to save checkpoint '{name}': {e}")
            return None

    def load(self, name: str):
        """
        Carga objeto desde checkpoint

        Args:
            name: Nombre del checkpoint ('tuning', 'selection', 'dataframes')

        Returns:
            Objeto cargado o None si no existe
        """
        filepath = self.checkpoint_dir / f"{name}.pkl"

        if not filepath.exists():
            logger.warning(f"Checkpoint '{name}' not found: {filepath}")
            return None

        try:
            with open(filepath, 'rb') as f:
                obj = pickle.load(f)

            size_mb = filepath.stat().st_size / 1024 / 1024
            logger.info(f"‚úì Checkpoint loaded: {name} ({size_mb:.2f} MB)")
            return obj

        except Exception as e:
            logger.error(f"‚úó Failed to load checkpoint '{name}': {e}")
            return None

    def exists(self, name: str) -> bool:
        """Verifica si checkpoint existe"""
        filepath = self.checkpoint_dir / f"{name}.pkl"
        return filepath.exists()

    def list_checkpoints(self):
        """Lista todos los checkpoints disponibles"""
        checkpoints = []
        for filepath in self.checkpoint_dir.glob("*.pkl"):
            size_mb = filepath.stat().st_size / 1024 / 1024
            modified = datetime.fromtimestamp(filepath.stat().st_mtime)
            checkpoints.append({
                'name': filepath.stem,
                'size_mb': size_mb,
                'modified': modified.strftime("%Y-%m-%d %H:%M:%S")
            })

        if checkpoints:
            logger.info(f"\n{'='*60}")
            logger.info("AVAILABLE CHECKPOINTS:")
            logger.info(f"{'='*60}")
            for cp in checkpoints:
                logger.info(f"  ‚Ä¢ {cp['name']:<20} {cp['size_mb']:>8.2f} MB    {cp['modified']}")
            logger.info(f"{'='*60}\n")
        else:
            logger.info("No checkpoints found.")

        return checkpoints

    def delete(self, name: str):
        """Elimina checkpoint"""
        filepath = self.checkpoint_dir / f"{name}.pkl"

        if filepath.exists():
            filepath.unlink()
            logger.info(f"‚úì Checkpoint deleted: {name}")
        else:
            logger.warning(f"Checkpoint '{name}' not found")

# Crear instancia global
checkpoint = CheckpointManager()

# Listar checkpoints existentes
checkpoint.list_checkpoints()

logger.info("CheckpointManager loaded successfully")
print("‚úì Celda 10.5 completada: CheckpointManager")
print("\nUso:")
print("  checkpoint.save('tuning', tuning_result)")
print("  checkpoint.save('selection', selection_result)")
print("  checkpoint.save('dataframes', {'train': train_df_long_scaled, 'holdout': holdout_df_long_scaled})")
print("\n  tuning_result = checkpoint.load('tuning')")
print("  checkpoint.exists('tuning')  # True/False")
print("  checkpoint.list_checkpoints()")

# ============================================================================
# CELDA 11: NBEATSxVisualizer (h=1 VERSION)
# ============================================================================

class NBEATSxVisualizer:
    """
    Generador de visualizaciones para NBEATSx h=1
    """

    @staticmethod
    def plot_predictions(dates, y_true, y_pred, title, ylabel, filename):
        """Gr√°fico de predicciones vs real"""
        plt.figure(figsize=(14, 6))
        plt.plot(dates, y_true, label='Real', color='black', linewidth=2)
        plt.plot(dates, y_pred, label='NBEATSx h=1', color='red', linewidth=2, linestyle='--')
        plt.title(title, fontsize=14, fontweight='bold')
        plt.xlabel('Fecha', fontsize=12)
        plt.ylabel(ylabel, fontsize=12)
        plt.legend(fontsize=11)
        plt.grid(alpha=0.3)
        plt.tight_layout()

        filepath = OUTPUT_DIR / filename
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"‚úì Plot saved: {filepath}")
        return filepath

    @staticmethod
    def plot_directional_accuracy(dates, y_true_ret, y_pred_ret):
        """Gr√°fico de precisi√≥n direccional d√≠a a d√≠a"""
        correct = (np.sign(y_true_ret) == np.sign(y_pred_ret)).astype(int)

        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))

        # Panel 1: Returns
        ax1.plot(dates, y_true_ret, label='Real Returns', color='black', linewidth=1.5)
        ax1.plot(dates, y_pred_ret, label='Predicted Returns', color='red', linewidth=1.5, linestyle='--')
        ax1.axhline(0, color='gray', linestyle='-', linewidth=0.5)
        ax1.set_title('Returns: Real vs Predicted (h=1)', fontsize=14, fontweight='bold')
        ax1.set_ylabel('Log Returns', fontsize=12)
        ax1.legend(fontsize=11)
        ax1.grid(alpha=0.3)

        # Panel 2: Directional Accuracy
        colors = ['green' if c == 1 else 'red' for c in correct]
        ax2.bar(dates, correct, color=colors, alpha=0.6, width=0.8)
        ax2.set_title('Directional Accuracy (Green=Correct, Red=Incorrect)',
                     fontsize=14, fontweight='bold')
        ax2.set_ylabel('Correct (1) / Incorrect (0)', fontsize=12)
        ax2.set_xlabel('Fecha', fontsize=12)
        ax2.set_ylim(-0.1, 1.1)
        ax2.grid(alpha=0.3)

        # Agregar DA promedio
        da_avg = np.mean(correct) * 100
        ax2.text(0.02, 0.95, f'DA: {da_avg:.2f}%',
                transform=ax2.transAxes, fontsize=12,
                verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

        plt.tight_layout()

        filepath = OUTPUT_DIR / 'directional_accuracy_h1.png'
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"‚úì Plot saved: {filepath}")
        return filepath

    @staticmethod
    def plot_exog_comparison(selection_results):
        """Gr√°fico de comparaci√≥n de ex√≥genas (MASE vs DA)"""
        exogs = [r['exog'] for r in selection_results]
        das = [r['DA'] for r in selection_results]
        mases = [r['MASE'] for r in selection_results]

        # Filtrar infinitos
        valid_idx = [i for i, m in enumerate(mases) if m < 10]
        exogs = [exogs[i] for i in valid_idx]
        das = [das[i] for i in valid_idx]
        mases = [mases[i] for i in valid_idx]

        fig, ax = plt.subplots(figsize=(12, 8))

        # Scatter plot
        scatter = ax.scatter(mases, das, s=100, alpha=0.6, c=range(len(exogs)), cmap='viridis')

        # Anotar puntos
        for i, exog in enumerate(exogs):
            ax.annotate(exog, (mases[i], das[i]),
                       fontsize=8, alpha=0.7,
                       xytext=(5, 5), textcoords='offset points')

        # L√≠neas de referencia
        ax.axhline(BASELINE_ARX['DA'], color='blue', linestyle='--',
                  linewidth=2, label=f"ARX DA: {BASELINE_ARX['DA']:.2f}%", alpha=0.7)
        ax.axhline(BASELINE_NHITS['DA'], color='orange', linestyle='--',
                  linewidth=2, label=f"NHITS DA: {BASELINE_NHITS['DA']:.2f}%", alpha=0.7)
        ax.axvline(BASELINE_ARX['MASE'], color='blue', linestyle=':',
                  linewidth=2, label=f"ARX MASE: {BASELINE_ARX['MASE']:.4f}", alpha=0.7)
        ax.axvline(BASELINE_NHITS['MASE'], color='orange', linestyle=':',
                  linewidth=2, label=f"NHITS MASE: {BASELINE_NHITS['MASE']:.4f}", alpha=0.7)

        ax.set_xlabel('MASE (Mean Absolute Scaled Error)', fontsize=12)
        ax.set_ylabel('DA (Directional Accuracy %)', fontsize=12)
        ax.set_title('Exogenous Variables Comparison: MASE vs DA (h=1)',
                    fontsize=14, fontweight='bold')
        ax.legend(fontsize=10, loc='best')
        ax.grid(alpha=0.3)

        plt.tight_layout()

        filepath = OUTPUT_DIR / 'exog_comparison_h1.png'
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"‚úì Plot saved: {filepath}")
        return filepath

    @staticmethod
    def generate_all_plots(final_result, selection_result):
        """Genera todos los gr√°ficos para h=1"""
        logger.info("="*80)
        logger.info("GENERATING VISUALIZATIONS (h=1)")
        logger.info("="*80)

        preds = final_result['predictions']
        dates = pd.to_datetime(preds['dates'])

        # 1. Predictions in levels
        plot1 = NBEATSxVisualizer.plot_predictions(
            dates=dates,
            y_true=preds['y_true_level'],
            y_pred=preds['y_pred_level'],
            title='NBEATSx h=1 Predictions: USD/PEN Levels (Holdout)',
            ylabel='USD/PEN',
            filename='predictions_levels_h1.png'
        )

        # 2. Predictions in returns
        y_true_ret = FXMetrics.levels_to_returns(preds['y_true_level'], preds['y_prev_level'])
        y_pred_ret = FXMetrics.levels_to_returns(preds['y_pred_level'], preds['y_prev_level'])

        plot2 = NBEATSxVisualizer.plot_predictions(
            dates=dates,
            y_true=y_true_ret,
            y_pred=y_pred_ret,
            title='NBEATSx h=1 Predictions: USD/PEN Log Returns (Holdout)',
            ylabel='Log Returns',
            filename='predictions_returns_h1.png'
        )

        # 3. Directional accuracy
        plot3 = NBEATSxVisualizer.plot_directional_accuracy(
            dates=dates,
            y_true_ret=y_true_ret,
            y_pred_ret=y_pred_ret
        )

        # 4. Exog comparison
        plot4 = NBEATSxVisualizer.plot_exog_comparison(
            selection_results=selection_result['all_results']
        )

        plots = {
            'predictions_levels': plot1,
            'predictions_returns': plot2,
            'directional_accuracy': plot3,
            'exog_comparison': plot4
        }

        logger.info("‚úì All visualizations generated")
        logger.info("="*80)

        return plots

logger.info("NBEATSxVisualizer class loaded successfully (h=1 version)")
print("‚úì Celda 11 completada: NBEATSxVisualizer h=1")

# ============================================================================
# CELDA 12: Cargar y Preparar Datos
# ============================================================================

# Cargar datos
df_raw = pd.read_csv('/content/DATA.csv')
logger.info(f"Data loaded: {df_raw.shape}")
print(f"Shape: {df_raw.shape}")

# Feature engineering
engineer = FeatureEngineer()
df_features = engineer.create_features(df_raw)

# Extraer all_exog_features
all_exog_features = df_features.attrs.get('all_exog_features', [])
print(f"\nTotal exogenous features: {len(all_exog_features)}")

# Clasificar ex√≥genas para scaling
exog_to_scale = [f for f in [
    'GARCH_vol', 'MXN_lag_1', 'CLP_lag_1', 'COBRE_lag_1', 'MXPE_lag_1',
    'UST10Y_lag_1', 'DXY_lag_1', 'CPI_lag_1', 'RESERV_lag_1', 'T_TRADE_lag_1', 'Tasa_cds_lag_1'
] if f in all_exog_features]

exog_no_scale = [f for f in [
    'RSI', 'MACD_line', 'month_sin', 'month_cos',
    'day_of_week_sin', 'day_of_week_cos', 'quarter_sin', 'quarter_cos'
] if f in all_exog_features]

print(f"Exog to scale: {len(exog_to_scale)}")
print(f"Exog no scale: {len(exog_no_scale)}")
print("\n‚úì Celda 12 completada")

# ============================================================================
# CELDA 12.1: CARGAR DATAFRAMES DESDE CHECKPOINT (Si existen)
# ============================================================================

print("="*80)
print("VERIFICANDO CHECKPOINT DE DATAFRAMES")
print("="*80)

# ‚úÖ Verificar si existe checkpoint de DataFrames
if checkpoint.exists('dataframes'):
    print("\n‚ö° CHECKPOINT ENCONTRADO - Cargando DataFrames desde disco...")
    print("   (Esto evita re-ejecutar Celdas 13-14)")

    # Cargar checkpoint
    dfs = checkpoint.load('dataframes')

    # Extraer variables
    train_df_long_scaled = dfs['train_df_long_scaled']
    holdout_df_long_scaled = dfs['holdout_df_long_scaled']
    train_df = dfs['train_df']
    holdout_df = dfs['holdout_df']
    df_features = dfs['df_features']
    all_exog_features = dfs['all_exog_features']
    train_returns = dfs['train_returns']

    # Verificar que todo se carg√≥ correctamente
    print(f"\n‚úì DataFrames cargados exitosamente:")
    print(f"  ‚Ä¢ train_df_long_scaled: {train_df_long_scaled.shape}")
    print(f"  ‚Ä¢ holdout_df_long_scaled: {holdout_df_long_scaled.shape}")
    print(f"  ‚Ä¢ train_df: {train_df.shape}")
    print(f"  ‚Ä¢ holdout_df: {holdout_df.shape}")
    print(f"  ‚Ä¢ df_features: {df_features.shape}")
    print(f"  ‚Ä¢ all_exog_features: {len(all_exog_features)} variables")
    print(f"  ‚Ä¢ train_returns: {len(train_returns)} obs")

    print(f"\n‚ö†Ô∏è IMPORTANTE: Puedes SALTAR Celdas 13 y 14 (ya est√°n en memoria)")
    print(f"              Contin√∫a directamente en Celda 15 (Tuning)")

    print(f"\nüí° Para forzar re-procesamiento:")
    print(f"   checkpoint.delete('dataframes')")
    print(f"   Luego ejecuta Celdas 12, 13, 14")

    # Flag para indicar que los DFs ya est√°n cargados
    DATAFRAMES_LOADED_FROM_CHECKPOINT = True

else:
    print("\n‚ö†Ô∏è NO HAY CHECKPOINT - Necesitas ejecutar Celdas 13 y 14")
    print(f"\nüìã Pasos siguientes:")
    print(f"   1. Ejecuta Celda 13 (Split train/holdout)")
    print(f"   2. Ejecuta Celda 14 (Conversi√≥n long + scaling)")
    print(f"   3. [OPCIONAL] Ejecuta Celda 14.5 (Guardar checkpoint)")
    print(f"   4. Luego contin√∫a con Celda 15 (Tuning)")

    # Flag para indicar que necesitamos procesar
    DATAFRAMES_LOADED_FROM_CHECKPOINT = False

print("="*80)
print(f"Checkpoint status: {'‚úì CARGADO' if DATAFRAMES_LOADED_FROM_CHECKPOINT else '‚úó NO DISPONIBLE'}")
print("="*80)
print("\n‚úì Celda 12.1 completada")

# ============================================================================
# CELDA 13: Split Train/Holdout (h=1)
# ============================================================================

# ‚úÖ Verificar si ya est√°n cargados desde checkpoint
if 'DATAFRAMES_LOADED_FROM_CHECKPOINT' in globals() and DATAFRAMES_LOADED_FROM_CHECKPOINT:
    print("‚ö†Ô∏è SALTANDO CELDA 13 - DataFrames ya cargados desde checkpoint")
    print("   train_df y holdout_df ya est√°n en memoria")
    print("\n‚úì Celda 13 completada (skipped)")
else:
    # ‚úÖ C√≥digo original de la Celda 13
    n_total = len(df_features)
    n_train = n_total - N_HOLDOUT
    n_holdout = N_HOLDOUT

    train_df = df_features.iloc[:n_train].copy()
    holdout_df = df_features.iloc[n_train:].copy()

    logger.info(f"Train: {len(train_df)} obs")
    logger.info(f"Holdout: {len(holdout_df)} obs")

    print(f"Total: {n_total} obs")
    print(f"Train: {n_train} obs ({n_train/n_total*100:.1f}%)")
    print(f"Holdout: {n_holdout} obs ({n_holdout/n_total*100:.1f}%)")
    print(f"\nTrain: {train_df.index.min()} to {train_df.index.max()}")
    print(f"Holdout: {holdout_df.index.min()} to {holdout_df.index.max()}")

    # Train returns para MASE
    train_returns = np.log(train_df['PEN_target'] / train_df['PEN_lag_1']).dropna().values
    print(f"\n‚úì Celda 13 completada")

# ============================================================================
# CELDA 14: Conversi√≥n Long + Scaling
# ============================================================================

# ‚úÖ Verificar si ya est√°n cargados desde checkpoint
if 'DATAFRAMES_LOADED_FROM_CHECKPOINT' in globals() and DATAFRAMES_LOADED_FROM_CHECKPOINT:
    print("‚ö†Ô∏è SALTANDO CELDA 14 - DataFrames ya cargados desde checkpoint")
    print("   train_df_long_scaled y holdout_df_long_scaled ya est√°n en memoria")
    print("\n‚úì Celda 14 completada (skipped)")
else:
    # ‚úÖ C√≥digo original de la Celda 14
    formatter = NeuralForecastFormatter()

    # Convertir a long
    train_df_long = formatter.format_to_long(train_df, 'PEN_target', all_exog_features)
    holdout_df_long = formatter.format_to_long(holdout_df, 'PEN_target', all_exog_features)

    # Fit scaler en train
    formatter.fit_scaler(train_df_long, exog_to_scale, exog_no_scale)

    # Transform
    train_df_long_scaled = formatter.transform_scaler(train_df_long)
    holdout_df_long_scaled = formatter.transform_scaler(holdout_df_long)

    # Validar
    formatter.validate_long_format(train_df_long_scaled)
    formatter.validate_long_format(holdout_df_long_scaled)

    print("‚úì Celda 14 completada")

# ============================================================================
# CELDA 14.5: GUARDAR CHECKPOINT DE DATAFRAMES
# ============================================================================

print("="*80)
print("GUARDANDO CHECKPOINT DE DATAFRAMES")
print("="*80)

# ‚úÖ Solo guardar si NO fueron cargados desde checkpoint
if 'DATAFRAMES_LOADED_FROM_CHECKPOINT' in globals() and DATAFRAMES_LOADED_FROM_CHECKPOINT:
    print("\n‚ö†Ô∏è SALTANDO - DataFrames ya ven√≠an de checkpoint")
    print("   No es necesario guardar nuevamente")

else:
    print("\nüíæ Guardando DataFrames procesados en checkpoint...")

    # Empaquetar todos los DFs y variables necesarias
    dataframes = {
        'train_df_long_scaled': train_df_long_scaled,
        'holdout_df_long_scaled': holdout_df_long_scaled,
        'train_df': train_df,
        'holdout_df': holdout_df,
        'df_features': df_features,
        'all_exog_features': all_exog_features,
        'train_returns': train_returns
    }

    # Guardar checkpoint
    checkpoint.save('dataframes', dataframes, force=True)

    print(f"\n‚úì Checkpoint guardado exitosamente")
    print(f"  Contenido:")
    print(f"  ‚Ä¢ train_df_long_scaled: {train_df_long_scaled.shape}")
    print(f"  ‚Ä¢ holdout_df_long_scaled: {holdout_df_long_scaled.shape}")
    print(f"  ‚Ä¢ train_df: {train_df.shape}")
    print(f"  ‚Ä¢ holdout_df: {holdout_df.shape}")
    print(f"  ‚Ä¢ df_features: {df_features.shape}")
    print(f"  ‚Ä¢ all_exog_features: {len(all_exog_features)} variables")
    print(f"  ‚Ä¢ train_returns: {len(train_returns)} obs")

    print(f"\nüí° Beneficio: Si Colab se desconecta, puedes recargar estos DFs")
    print(f"             instant√°neamente en Celda 12.1 (sin re-procesar)")

print("="*80)
print("\n‚úì Celda 14.5 completada")

# ============================================================================
# CELDA 15: Hyperparameter Tuning h=1 (CON CHECKPOINTING)
# ============================================================================

print("="*80)
print(f"HYPERPARAMETER TUNING h={H_FORECAST} - CON CHECKPOINTING")
print("="*80)

# ‚úÖ VERIFICAR SI YA EXISTE CHECKPOINT
if checkpoint.exists('tuning'):
    print("\n‚ö° CHECKPOINT ENCONTRADO - Cargando tuning_result desde disco...")
    print("   (Skip tuning para ahorrar tiempo)")

    # Preguntar al usuario si quiere usar checkpoint o re-ejecutar
    print("\n¬øQuieres usar el checkpoint existente?")
    print("  1. S√ç - Cargar checkpoint (instant√°neo)")
    print("  2. NO - Re-ejecutar tuning completo (~1-2 horas)")

    # Para Colab, asumimos que el usuario quiere usar checkpoint
    # Si quiere re-ejecutar, debe eliminar manualmente:
    # checkpoint.delete('tuning')

    use_checkpoint = True  # Cambiar a False para forzar re-ejecuci√≥n

    if use_checkpoint:
        tuning_result = checkpoint.load('tuning')

        print(f"\n‚úì Tuning result cargado desde checkpoint")
        print(f"  Best DA: {tuning_result['best_metrics']['DA_valid']:.2f}%")
        print(f"  Valid trials: {tuning_result['n_valid_trials']}")
        print("\nBest Config:")
        for key, val in tuning_result['best_config'].items():
            print(f"  {key}: {val}")

        print(f"\n‚ö†Ô∏è Para re-ejecutar tuning, ejecuta: checkpoint.delete('tuning')")
        print("   y luego vuelve a correr esta celda.")

    else:
        print("\n‚ö†Ô∏è Re-ejecutando tuning (borrando checkpoint anterior)...")
        checkpoint.delete('tuning')

        # Ejecutar tuning normal (c√≥digo de abajo)
        tuning_result = None

else:
    print("\n‚ö†Ô∏è No hay checkpoint - Ejecutando tuning por primera vez...")
    tuning_result = None

# ‚úÖ SI NO HAY CHECKPOINT, EJECUTAR TUNING
if tuning_result is None:
    print(f"\n‚è±Ô∏è Esto tomar√° ~1-2 horas con GPU A100...")
    print(f"Trials: {N_TRIALS} | Timeout: {TIMEOUT}s (~{TIMEOUT/3600:.1f}h)")

    tuner = NBEATSxTuner(h=H_FORECAST)

    tuning_result = tuner.tune(
        train_df_long=train_df_long_scaled,
        hist_exog_list=all_exog_features,
        n_trials=N_TRIALS,
        timeout=TIMEOUT
    )

    # ‚úÖ GUARDAR CHECKPOINT INMEDIATAMENTE
    print(f"\nüíæ Guardando checkpoint de tuning...")
    checkpoint.save('tuning', tuning_result, force=True)

    print(f"\n{'='*80}")
    print(f"TUNING COMPLETADO Y GUARDADO")
    print(f"{'='*80}")
    print(f"\nBest DA (validation): {tuning_result['best_metrics']['DA_valid']:.2f}%")
    print(f"Valid trials: {tuning_result['n_valid_trials']}/{N_TRIALS}")

# Validaci√≥n anti-leakage (com√∫n para checkpoint o nuevo)
if tuning_result['best_metrics']['DA_valid'] > 70:
    print(f"\nüö® WARNING: DA={tuning_result['best_metrics']['DA_valid']:.2f}% is still too high!")
elif tuning_result['best_metrics']['DA_valid'] > 48:
    print(f"\n‚úì DA={tuning_result['best_metrics']['DA_valid']:.2f}% is in expected range")
else:
    print(f"\n‚ö†Ô∏è DA={tuning_result['best_metrics']['DA_valid']:.2f}% is low (<48%)")

print("\n‚úì Celda 15 completada")

# ============================================================================
# CELDA 16: Selecci√≥n Ex√≥genas h=1 (CON CHECKPOINTING)
# ============================================================================

print("="*80)
print(f"SELECCI√ìN EXHAUSTIVA DE EX√ìGENAS h={H_FORECAST}")
print("="*80)

# ‚úÖ VERIFICAR SI YA EXISTE CHECKPOINT
if checkpoint.exists('selection'):
    print("\n‚ö° CHECKPOINT ENCONTRADO - Cargando selection_result...")

    use_checkpoint = True  # Cambiar a False para forzar re-ejecuci√≥n

    if use_checkpoint:
        selection_result = checkpoint.load('selection')

        print(f"\n‚úì Selection result cargado desde checkpoint")
        print(f"  Best exog: {selection_result['best_exog']}")
        print(f"  Best DA: {selection_result['best_metrics']['DA']:.2f}%")

        print(f"\n‚ö†Ô∏è Para re-ejecutar selection, ejecuta: checkpoint.delete('selection')")

    else:
        checkpoint.delete('selection')
        selection_result = None

else:
    print("\n‚ö†Ô∏è No hay checkpoint - Ejecutando selection por primera vez...")
    selection_result = None

# ‚úÖ SI NO HAY CHECKPOINT, EJECUTAR SELECTION
if selection_result is None:
    print(f"\nCandidatas: {len(all_exog_features) + 1}")
    print(f"‚è±Ô∏è Esto tomar√° ~30-60 minutos...")

    selector = ExogenousSelector(h=H_FORECAST)
    exog_candidates = ['NONE'] + all_exog_features

    selection_result = selector.select(
        train_df_long=train_df_long_scaled,
        train_df_wide=train_df,
        exog_candidates=exog_candidates,
        best_config=tuning_result['best_config']
    )

    # ‚úÖ GUARDAR CHECKPOINT
    print(f"\nüíæ Guardando checkpoint de selection...")
    checkpoint.save('selection', selection_result, force=True)

    print(f"\n{'='*80}")
    print(f"SELECTION COMPLETADA Y GUARDADA")
    print(f"{'='*80}")

# Mostrar resultados (com√∫n)
print(f"\nBest Exogenous: {selection_result['best_exog']}")
print(f"DA: {selection_result['best_metrics']['DA']:.2f}%")
print(f"MASE: {selection_result['best_metrics']['MASE']:.4f}")

print("\n‚úì TOP 5:")
all_results_sorted = sorted(selection_result['all_results'],
                            key=lambda x: (-x['DA'], x['MASE']))
for i, res in enumerate(all_results_sorted[:5]):
    print(f"  {i+1}. {res['exog']:<15} ‚Üí DA={res['DA']:>5.2f}%, MASE={res['MASE']:>6.4f}")

print("\n‚úì Celda 16 completada")

# ============================================================================
# CELDA 17: Entrenamiento Final y Evaluaci√≥n Holdout h=1 (CORRECTED)
# ============================================================================

print("="*80)
print(f"EVALUACI√ìN FINAL EN HOLDOUT h={H_FORECAST} - SIN LEAKAGE")
print("="*80)

evaluator = FinalEvaluator(h=H_FORECAST)

final_result = evaluator.train_and_evaluate(
    train_df_long=train_df_long_scaled,
    holdout_df_long=holdout_df_long_scaled,
    full_df=df_features,
    best_config=tuning_result['best_config'],
    best_exog=selection_result['best_exog'],
    train_returns=train_returns
)

print("\n" + "="*80)
print("M√âTRICAS HOLDOUT:")
print(f"  DA: {final_result['holdout_metrics']['DA']:.2f}%")
print(f"  MASE: {final_result['holdout_metrics']['MASE']:.4f}")
print(f"\nCOMPARACI√ìN (h=1 vs h=1):")
print(f"  vs ARX: ŒîDA={final_result['comparison']['delta_vs_arx']['DA']:+.2f}%")
print(f"  vs NHITS: ŒîDA={final_result['comparison']['delta_vs_nhits']['DA']:+.2f}%")
print(f"\nVEREDICTO: {final_result['comparison']['verdict']}")
print("\n‚úì Celda 17 completada")

# MODIFICACI√ìN 5: CELDA 17.5 (NUEVA) - Ejecutar generaci√≥n de OOF
# ============================================================================
# CREAR NUEVA CELDA despu√©s de CELDA 17 (Final Evaluation) y antes de CELDA 18
# COPIAR TODO ESTE C√ìDIGO:
# ----------------------------------------------------------------------------

print("\n" + "="*80)
print("üîÑ GENERANDO OOF PREDICTIONS PARA META-LEARNER")
print("="*80)

# Generar OOF usando Walk-Forward
oof_results = generate_oof_predictions_nbeatsx(
    train_df_long_scaled=train_df_long_scaled,
    train_df_wide=train_df,
    best_config=tuning_result['best_config'],
    best_exog=selection_result['best_exog'],
    train_returns=train_returns,
    checkpoint_dir=OOF_DIR
)

print("\n‚úì Celda 17.5 completada: OOF predictions generadas")

# ============================================================================
# CELDA 18: Persistencia
# ============================================================================

persistence = ModelPersistence()

# Guardar modelo
model_path = persistence.save_model(final_result['model'], 'nbeatsx_h1_final.pkl')

# Guardar m√©tricas
metrics_path = persistence.save_metrics(final_result, tuning_result, selection_result)

# Actualizar train_size
with open(metrics_path, 'r') as f:
    metrics_json = json.load(f)
metrics_json['metadata']['train_size'] = len(train_df)
with open(metrics_path, 'w') as f:
    json.dump(metrics_json, f, indent=2)

# Guardar config
config_path = persistence.save_config(tuning_result['best_config'], selection_result['best_exog'])

# Crear README
readme_path = persistence.create_readme(final_result)

print(f"\n‚úì Archivos guardados en: {OUTPUT_DIR}")
print("\n‚úì Celda 18 completada")

# ============================================================================
# CELDA 19: Visualizaciones
# ============================================================================

visualizer = NBEATSxVisualizer()

plots = visualizer.generate_all_plots(final_result, selection_result)

print("\n‚úì Visualizaciones generadas:")
for name, path in plots.items():
    print(f"  - {name}: {path.name}")

# Mostrar gr√°ficos
from IPython.display import Image, display

print("\n1. PREDICTIONS - LEVELS")
display(Image(filename=plots['predictions_levels']))

print("\n2. PREDICTIONS - RETURNS")
display(Image(filename=plots['predictions_returns']))

print("\n3. DIRECTIONAL ACCURACY")
display(Image(filename=plots['directional_accuracy']))

print("\n4. EXOGENOUS COMPARISON")
display(Image(filename=plots['exog_comparison']))

print("\n‚úì Celda 19 completada")

# ============================================================================
# CELDA 20: RESUMEN FINAL
# ============================================================================

print("="*80)
print("EXPERIMENTO NBEATSx h=1 - RESUMEN FINAL")
print("="*80)

print("\nüìä RESULTADOS FINALES:")
print(f"  ‚Ä¢ DA: {final_result['holdout_metrics']['DA']:.2f}%")
print(f"  ‚Ä¢ MASE: {final_result['holdout_metrics']['MASE']:.4f}")
print(f"  ‚Ä¢ Best Exog: {selection_result['best_exog']}")
print(f"\nüìä RESULTADOS OOF (para Meta-Learner):")
print(f"  ‚Ä¢ DA OOF:   {oof_results['metrics']['da']:.2f}%")
print(f"  ‚Ä¢ MASE OOF: {oof_results['metrics']['mase']:.4f}")
print(f"  ‚Ä¢ Observaciones OOF: {len(oof_results['predictions'])}")


print("\nüìà COMPARACI√ìN VS BASELINES (TODOS h=1):")
print(f"  ‚Ä¢ vs ARX: ŒîDA = {final_result['comparison']['delta_vs_arx']['DA']:+.2f}%")
print(f"  ‚Ä¢ vs NHITS: ŒîDA = {final_result['comparison']['delta_vs_nhits']['DA']:+.2f}%")

print(f"\nüéØ VEREDICTO: {final_result['comparison']['verdict']}")

print("\n‚öôÔ∏è CONFIGURACI√ìN √ìPTIMA:")
for key, val in tuning_result['best_config'].items():
    print(f"  ‚Ä¢ {key}: {val}")

print("\nüìÅ ARCHIVOS GENERADOS:")
print(f"  ‚Ä¢ Modelo: nbeatsx_h1_final.pkl")
print(f"  ‚Ä¢ M√©tricas: metrics.json")
print(f"  ‚Ä¢ Config: config.json")
print(f"  ‚Ä¢ README: README.md")
print(f"  ‚Ä¢ Checkpoints: tuning.pkl, selection.pkl, dataframes.pkl")
print(f"  ‚Ä¢ Gr√°ficos: 4 archivos PNG")

print("\n‚úÖ EXPERIMENTO COMPLETADO")
print("="*80)

# ‚úÖ Validaci√≥n estad√≠stica (CORREGIDA)
print(f"\nüìä SIGNIFICANCIA ESTAD√çSTICA:")

try:
    # ‚úÖ CORRECCI√ìN: Usar binomtest (sin gui√≥n bajo)
    from scipy.stats import binomtest

    n_preds = len(final_result['predictions']['y_true_level'])
    n_correct = int(final_result['holdout_metrics']['DA'] * n_preds / 100)

    # Ejecutar test binomial
    result = binomtest(n_correct, n_preds, 0.5, alternative='greater')
    p_value = result.pvalue

    print(f"  ‚Ä¢ N predicciones: {n_preds}")
    print(f"  ‚Ä¢ Correctas: {n_correct}")
    print(f"  ‚Ä¢ p-value: {p_value:.4f}")

    if p_value < 0.05:
        print(f"  ‚Ä¢ ‚úì DA significativamente > 50% (p < 0.05)")
        print(f"    Conclusi√≥n: El modelo tiene capacidad predictiva estad√≠sticamente significativa")
    elif p_value < 0.10:
        print(f"  ‚Ä¢ ~ DA marginalmente > 50% (0.05 < p < 0.10)")
        print(f"    Conclusi√≥n: Evidencia d√©bil de capacidad predictiva")
    else:
        print(f"  ‚Ä¢ ‚úó DA NO significativamente > 50% (p >= 0.10)")
        print(f"    Conclusi√≥n: No se puede rechazar hip√≥tesis nula (modelo = random walk)")

except ImportError as e:
    print(f"  ‚ö†Ô∏è scipy.stats no disponible para test estad√≠stico")
    print(f"     (Esto no afecta los resultados del modelo)")

    # An√°lisis manual sin scipy
    n_preds = len(final_result['predictions']['y_true_level'])
    n_correct = int(final_result['holdout_metrics']['DA'] * n_preds / 100)

    print(f"\n  üìä AN√ÅLISIS DESCRIPTIVO:")
    print(f"  ‚Ä¢ N predicciones: {n_preds}")
    print(f"  ‚Ä¢ Correctas: {n_correct}")
    print(f"  ‚Ä¢ DA: {final_result['holdout_metrics']['DA']:.2f}%")

    # Regla emp√≠rica (sin p-value)
    if final_result['holdout_metrics']['DA'] > 55:
        print(f"  ‚Ä¢ ‚úì DA > 55%: Muy probablemente significativo")
    elif final_result['holdout_metrics']['DA'] > 52:
        print(f"  ‚Ä¢ ~ DA > 52%: Posiblemente significativo (depende de n)")
    else:
        print(f"  ‚Ä¢ ? DA ‚âà 50-52%: Dif√≠cil determinar sin test estad√≠stico")

print("\n‚úì Celda 20 completada")

print("\nüéì PARA TU TESIS:")
print("  1. Revisa README.md para interpretaci√≥n completa")
print("  2. Compara m√©tricas con ARX y NHITS (metrics.json)")
print("  3. Analiza gr√°ficos de predicciones y DA")
print("  4. Discute limitaciones de h=1 (solo identity stacks)")
print("  5. Reporta metodolog√≠a anti-leakage (train-valid split, rolling forecast)")
print("  6. Interpreta resultado honestamente (puede ser superior, mixto, o inferior)")

# Interpretaci√≥n final basada en DA
da_final = final_result['holdout_metrics']['DA']

print(f"\nüí° INTERPRETACI√ìN FINAL:")
if da_final > BASELINE_NHITS['DA'] and da_final > BASELINE_ARX['DA']:
    print(f"  üéâ NBEATSx SUPERA ambos baselines (DA={da_final:.2f}%)")
    print(f"     Contribuci√≥n: Las arquitecturas basis expansion pueden")
    print(f"     capturar patrones no-lineales en mercados FX eficientes.")
elif da_final > BASELINE_NHITS['DA']:
    print(f"  ‚úì NBEATSx SUPERA NHITS pero no ARX (DA={da_final:.2f}%)")
    print(f"    Contribuci√≥n: Mejor que pooling jer√°rquico, pero modelos")
    print(f"    lineales siguen siendo competitivos en FX.")
elif da_final > 52:
    print(f"  ~ NBEATSx tiene capacidad predictiva (DA={da_final:.2f}%)")
    print(f"    pero no supera baselines establecidos.")
    print(f"    Contribuci√≥n: Validaci√≥n de eficiencia de mercado.")
else:
    print(f"  ‚ö†Ô∏è NBEATSx no mejora sobre random walk (DA={da_final:.2f}%)")
    print(f"     Contribuci√≥n: Evidencia de limitaciones de deep learning")
    print(f"     en mercados altamente eficientes como USD/PEN.")

print("\n¬°√âXITO! üöÄ")
print("\nTodos los archivos est√°n guardados en:")
print(f"  {OUTPUT_DIR}")
print(f"  ‚Ä¢ OOF Train: {OOF_DIR}/train_oof_NBEATSx.csv (~{len(oof_results['predictions'])} obs)")

# ============================================================================
# EXPORTAR PREDICCIONES PARA META-LEARNER
# ============================================================================

print("\n" + "="*80)
print("EXPORTANDO PREDICCIONES PARA META-LEARNER")
print("="*80)

# Extraer predicciones
y_pred_level = final_result['predictions']['y_pred_level']
y_true_level = final_result['predictions']['y_true_level']
y_prev_level = final_result['predictions']['y_prev_level']
holdout_dates = final_result['predictions']['dates']

# Convertir niveles a log returns (como se hace en la evaluaci√≥n)
y_pred_returns = np.log(y_pred_level / y_prev_level)

# Validar 60 predicciones
assert len(y_pred_returns) == 60, f"Error: Se esperaban 60 predicciones, se obtuvieron {len(y_pred_returns)}"

print(f"\nüìä ESTAD√çSTICAS DE RETURNS:")
print(f"  Media: {y_pred_returns.mean():.6f}")
print(f"  Std: {y_pred_returns.std():.6f}")
print(f"  Min: {y_pred_returns.min():.6f}")
print(f"  Max: {y_pred_returns.max():.6f}")

# ‚úÖ CORRECCI√ìN: Guardar en Google Drive (ubicaci√≥n permanente)
predictions_dir = '/content/drive/MyDrive/Colab_Outputs/predictions_dump'

# Exportar returns (no niveles)
export_model_predictions(
    dates=holdout_dates,
    y_pred=y_pred_returns,
    model_name='NBEATSx',
    prediction_type='log_returns',
    output_dir=predictions_dir  # ‚Üê Guardar en Drive
)

print(f"‚úÖ Predicciones exportadas en: {predictions_dir}/pred_NBEATSx.csv")
print("="*80)