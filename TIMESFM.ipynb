{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "305bda74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " See https://github.com/google-research/timesfm/blob/master/README.md for updated APIs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:19:51 - INFO - ================================================================================\n",
      "2025-11-26 15:19:51 - INFO - TIMESFM USD/PEN FORECASTING - LOCAL WINDOWS\n",
      "2025-11-26 15:19:51 - INFO - ================================================================================\n",
      "2025-11-26 15:19:51 - INFO - PyTorch version: 2.5.1+cu121\n",
      "2025-11-26 15:19:51 - INFO - CUDA available: True\n",
      "2025-11-26 15:19:51 - INFO - CUDA device: NVIDIA GeForce RTX 2060\n",
      "2025-11-26 15:19:51 - INFO - ================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded PyTorch TimesFM, likely because python version is 3.10.19 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 16:41:31) [MSC v.1929 64 bit (AMD64)].\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# TIMESFM h=1 USD/PEN FORECASTING - LOCAL (VS CODE)\n",
    "# ================================================================================\n",
    "# Modelo: TimesFM (Google) - 200M parameters\n",
    "# Dataset: DATA.csv (USD/PEN daily)\n",
    "# Target: PEN_log_ret[t] (h=1 one-step-ahead)\n",
    "# Environment: Jupyter Notebook - VS Code - Windows\n",
    "# ================================================================================\n",
    "\n",
    "# ================================================================================\n",
    "# CELDA 1: IMPORTS Y CONFIGURACI√ìN\n",
    "# ================================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import Tuple, Dict, List, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TimesFM espec√≠fico\n",
    "import timesfm\n",
    "import torch\n",
    "\n",
    "# Configuraci√≥n de logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Semilla aleatoria\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_STATE)\n",
    "\n",
    "logger.info(\"=\" * 80)\n",
    "logger.info(\"TIMESFM USD/PEN FORECASTING - LOCAL WINDOWS\")\n",
    "logger.info(\"=\" * 80)\n",
    "logger.info(f\"PyTorch version: {torch.__version__}\")\n",
    "logger.info(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    logger.info(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "logger.info(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0705fa28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:19:51 - INFO - Configuration loaded - VERSI√ìN NIVELES:\n",
      "2025-11-26 15:19:51 - INFO -   Target: PEN (NIVELES)\n",
      "2025-11-26 15:19:51 - INFO -   Base dir: C:\\Users\\Carlos Palma\\OneDrive\\Documents\\Cursos\\UTEC Computer Science\\TESIS\\NUEVO PAPER\\tesis_maestria\n",
      "2025-11-26 15:19:51 - INFO -   N_HOLDOUT: 60\n",
      "2025-11-26 15:19:51 - INFO -   H_FORECAST: 1\n",
      "2025-11-26 15:19:51 - INFO -   TimesFM model: google/timesfm-1.0-200m-pytorch\n",
      "2025-11-26 15:19:51 - INFO -   Context length: 512\n",
      "2025-11-26 15:19:51 - INFO -   Backend: gpu\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ================================================================================\n",
    "# CELDA 2: CONSTANTES Y CONFIGURACI√ìN - RUTAS WINDOWS LOCAL\n",
    "# ================================================================================\n",
    "\n",
    "# ============================================================================\n",
    "# üî¥ RUTAS WINDOWS - AJUSTAR SI ES NECESARIO\n",
    "# ============================================================================\n",
    "BASE_DIR = Path(r\"C:\\Users\\Carlos Palma\\OneDrive\\Documents\\Cursos\\UTEC Computer Science\\TESIS\\NUEVO PAPER\\tesis_maestria\")\n",
    "DATA_PATH = BASE_DIR / \"DATA.csv\"\n",
    "OUTPUT_DIR = BASE_DIR / \"TimesFM_h1_USD_PEN\"\n",
    "CHECKPOINT_DIR = OUTPUT_DIR / \"checkpoints\"\n",
    "PREDICTIONS_DUMP = BASE_DIR / \"predictions_dump\"\n",
    "OOF_DIR = BASE_DIR / \"oof_predictions\"\n",
    "\n",
    "# Crear directorios\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PREDICTIONS_DUMP.mkdir(parents=True, exist_ok=True)\n",
    "OOF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# ‚òÖ‚òÖ‚òÖ CONFIGURACI√ìN CORREGIDA: TARGET EN NIVELES ‚òÖ‚òÖ‚òÖ\n",
    "# ============================================================================\n",
    "TARGET_COL = 'PEN'              # ‚Üê NIVELES (antes era 'PEN_log_ret')\n",
    "FREQ = 'D'\n",
    "N_HOLDOUT = 60                  # üìå EXACTAMENTE 60 D√çAS\n",
    "H_FORECAST = 1                  # üìå HORIZONTE h=1\n",
    "MIN_TRAIN = 252                 # M√≠nimo 1 a√±o de entrenamiento\n",
    "STEP_SIZE = 21                  # Para walk-forward OOF\n",
    "\n",
    "# TimesFM Configuration\n",
    "TIMESFM_CONFIG = {\n",
    "    'model_id': 'google/timesfm-1.0-200m-pytorch',\n",
    "    'context_len': 512,         # 2 a√±os de datos diarios\n",
    "    'horizon_len': H_FORECAST,\n",
    "    'batch_size': 32,\n",
    "    'backend': 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "}\n",
    "\n",
    "# Baselines de referencia\n",
    "BASELINE_ARX = {'DA': 51.67, 'MASE': 0.9398}\n",
    "\n",
    "# Run ID\n",
    "RUN_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "logger.info(\"Configuration loaded - VERSI√ìN NIVELES:\")\n",
    "logger.info(f\"  Target: {TARGET_COL} (NIVELES)\")\n",
    "logger.info(f\"  Base dir: {BASE_DIR}\")\n",
    "logger.info(f\"  N_HOLDOUT: {N_HOLDOUT}\")\n",
    "logger.info(f\"  H_FORECAST: {H_FORECAST}\")\n",
    "logger.info(f\"  TimesFM model: {TIMESFM_CONFIG['model_id']}\")\n",
    "logger.info(f\"  Context length: {TIMESFM_CONFIG['context_len']}\")\n",
    "logger.info(f\"  Backend: {TIMESFM_CONFIG['backend']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53cc77bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:19:51 - INFO - ‚úì FXMetrics class defined (con conversi√≥n niveles‚Üíretornos)\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ================================================================================\n",
    "# CELDA 3: FXMetrics CLASS - ‚òÖ‚òÖ‚òÖ ADAPTADA PARA NIVELES‚ÜíRETORNOS ‚òÖ‚òÖ‚òÖ\n",
    "# ================================================================================\n",
    "\n",
    "class FXMetrics:\n",
    "    \"\"\"M√©tricas para evaluaci√≥n - Convierte niveles a retornos para c√°lculo\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def directional_accuracy_from_levels(y_true_levels: np.ndarray, \n",
    "                                          y_pred_levels: np.ndarray,\n",
    "                                          y_prev_levels: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calcular DA a partir de predicciones en NIVELES.\n",
    "        \n",
    "        Args:\n",
    "            y_true_levels: Niveles reales [t]\n",
    "            y_pred_levels: Niveles predichos [t]\n",
    "            y_prev_levels: Niveles del d√≠a anterior [t-1]\n",
    "            \n",
    "        Returns:\n",
    "            DA en porcentaje (0-100)\n",
    "        \"\"\"\n",
    "        # Convertir a retornos\n",
    "        true_returns = np.log(y_true_levels / y_prev_levels)\n",
    "        pred_returns = np.log(y_pred_levels / y_prev_levels)\n",
    "        \n",
    "        # Direcciones\n",
    "        true_dir = np.sign(true_returns)\n",
    "        pred_dir = np.sign(pred_returns)\n",
    "        \n",
    "        # Accuracy\n",
    "        correct = np.sum(true_dir == pred_dir)\n",
    "        total = len(true_dir)\n",
    "        \n",
    "        return (correct / total) * 100\n",
    "    \n",
    "    @staticmethod\n",
    "    def mase_from_levels(y_true_levels: np.ndarray,\n",
    "                         y_pred_levels: np.ndarray,\n",
    "                         y_prev_levels: np.ndarray,\n",
    "                         train_levels: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calcular MASE a partir de predicciones en NIVELES.\n",
    "        \n",
    "        Args:\n",
    "            y_true_levels: Niveles reales holdout\n",
    "            y_pred_levels: Niveles predichos\n",
    "            y_prev_levels: Niveles d√≠a anterior\n",
    "            train_levels: Niveles del train (para naive MAE)\n",
    "        \"\"\"\n",
    "        # Convertir a retornos\n",
    "        true_returns = np.log(y_true_levels / y_prev_levels)\n",
    "        pred_returns = np.log(y_pred_levels / y_prev_levels)\n",
    "        \n",
    "        # MAE del modelo\n",
    "        model_mae = np.mean(np.abs(true_returns - pred_returns))\n",
    "        \n",
    "        # MAE del naive (random walk en retornos)\n",
    "        train_returns = np.log(train_levels[1:] / train_levels[:-1])\n",
    "        naive_mae = np.mean(np.abs(train_returns))\n",
    "        \n",
    "        return model_mae / naive_mae if naive_mae > 0 else np.inf\n",
    "    \n",
    "    @staticmethod\n",
    "    def mae_from_levels(y_true_levels: np.ndarray,\n",
    "                        y_pred_levels: np.ndarray,\n",
    "                        y_prev_levels: np.ndarray) -> float:\n",
    "        \"\"\"MAE en espacio de retornos\"\"\"\n",
    "        true_returns = np.log(y_true_levels / y_prev_levels)\n",
    "        pred_returns = np.log(y_pred_levels / y_prev_levels)\n",
    "        return np.mean(np.abs(true_returns - pred_returns))\n",
    "\n",
    "    # Versiones legacy para compatibilidad (cuando ya tienes retornos)\n",
    "    @staticmethod\n",
    "    def calculate_da(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        \"\"\"DA directo con niveles (usando diferencias)\"\"\"\n",
    "        if len(y_true) != len(y_pred):\n",
    "            raise ValueError(f\"Length mismatch: y_true={len(y_true)}, y_pred={len(y_pred)}\")\n",
    "        \n",
    "        direction_true = np.sign(np.diff(y_true))\n",
    "        direction_pred = np.sign(np.diff(y_pred))\n",
    "        correct = np.sum(direction_true == direction_pred)\n",
    "        total = len(direction_true)\n",
    "        \n",
    "        return (correct / total) * 100\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_mase(y_true: np.ndarray, y_pred: np.ndarray, train_values: np.ndarray) -> float:\n",
    "        \"\"\"MASE directo\"\"\"\n",
    "        mae = np.mean(np.abs(y_true - y_pred))\n",
    "        naive_mae = np.mean(np.abs(np.diff(train_values)))\n",
    "        return mae / naive_mae if naive_mae > 0 else np.inf\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_mae(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        \"\"\"MAE simple\"\"\"\n",
    "        return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "logger.info(\"‚úì FXMetrics class defined (con conversi√≥n niveles‚Üíretornos)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4aa7195",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:19:51 - INFO - ‚úì TimesFMConverter class defined\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ================================================================================\n",
    "# CELDA 4: TimesFMConverter CLASS\n",
    "# ================================================================================\n",
    "\n",
    "class TimesFMConverter:\n",
    "    \"\"\"Convierte datos a formato TimesFM\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_to_timesfm_df(df: pd.DataFrame, \n",
    "                              value_col: str,\n",
    "                              series_id: str = 'series',\n",
    "                              freq: str = 'D') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convierte DataFrame a formato TimesFM.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame con DatetimeIndex\n",
    "            value_col: Columna con valores (ahora NIVELES)\n",
    "            series_id: Identificador de la serie\n",
    "            freq: Frecuencia ('D' para diario)\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame con columnas: unique_id, ds, y\n",
    "        \"\"\"\n",
    "        timesfm_df = pd.DataFrame({\n",
    "            'unique_id': series_id,\n",
    "            'ds': df.index,\n",
    "            'y': df[value_col].values\n",
    "        })\n",
    "        \n",
    "        logger.info(f\"Converted to TimesFM format:\")\n",
    "        logger.info(f\"  Rows: {len(timesfm_df)}\")\n",
    "        logger.info(f\"  Value column: {value_col} (NIVELES)\")\n",
    "        logger.info(f\"  Y range: [{timesfm_df['y'].min():.4f}, {timesfm_df['y'].max():.4f}]\")\n",
    "        \n",
    "        return timesfm_df\n",
    "    \n",
    "    @staticmethod\n",
    "    def split_timesfm_data(timesfm_df: pd.DataFrame,\n",
    "                          n_holdout: int) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Split TimesFM DataFrame en train/holdout\"\"\"\n",
    "        split_idx = len(timesfm_df) - n_holdout\n",
    "        \n",
    "        train_df = timesfm_df.iloc[:split_idx].copy()\n",
    "        holdout_df = timesfm_df.iloc[split_idx:].copy()\n",
    "        \n",
    "        logger.info(f\"Split data:\")\n",
    "        logger.info(f\"  Train: {len(train_df)} observations\")\n",
    "        logger.info(f\"  Holdout: {len(holdout_df)} observations\")\n",
    "        \n",
    "        return train_df, holdout_df\n",
    "\n",
    "logger.info(\"‚úì TimesFMConverter class defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a03762b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:19:51 - INFO - ‚úì TimesFMEvaluator class defined (VERSI√ìN NIVELES)\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ================================================================================\n",
    "# CELDA 5: TimesFMEvaluator CLASS - ‚òÖ‚òÖ‚òÖ PREDICE NIVELES ‚òÖ‚òÖ‚òÖ\n",
    "# ================================================================================\n",
    "\n",
    "class TimesFMEvaluator:\n",
    "    \"\"\"Evaluaci√≥n de TimesFM con rolling forecast - VERSI√ìN NIVELES\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model,\n",
    "                 train_df: pd.DataFrame,\n",
    "                 holdout_df: pd.DataFrame,\n",
    "                 full_df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: TimesFM model instance\n",
    "            train_df: Train data (TimesFM format)\n",
    "            holdout_df: Holdout data (TimesFM format)\n",
    "            full_df: Full data (TimesFM format) = train + holdout\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.train_df = train_df\n",
    "        self.holdout_df = holdout_df\n",
    "        self.full_df = full_df\n",
    "        \n",
    "    def rolling_forecast(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Rolling forecast h=1 en holdout - PREDICE NIVELES\n",
    "        \n",
    "        ============================================================\n",
    "        METODOLOG√çA CORRECTA - ROLLING FORECAST SIN REENTRENAMIENTO\n",
    "        ============================================================\n",
    "        \n",
    "        ‚úÖ V√ÅLIDO: fit(train) ‚Üí predict(train + holdout_observed_values)\n",
    "        ‚ùå LEAKAGE: fit(train + holdout) ‚Üí predict(...)\n",
    "        \n",
    "        Para cada d√≠a i en holdout:\n",
    "        - Contexto = train + holdout[:i] (valores REALES como INPUT)\n",
    "        - Predice d√≠a i+1 (SOLO INFERENCE, NO RETRAINING)\n",
    "        \n",
    "        El modelo ahora predice NIVELES directamente.\n",
    "        \n",
    "        Returns:\n",
    "            Dict con predictions (niveles) y metadata\n",
    "        \"\"\"\n",
    "        logger.info(\"=\" * 80)\n",
    "        logger.info(\"ROLLING FORECAST (TimesFM - NIVELES)\")\n",
    "        logger.info(\"=\" * 80)\n",
    "        \n",
    "        predictions_levels = []\n",
    "        n_holdout = len(self.holdout_df)\n",
    "        \n",
    "        logger.info(f\"Generating {n_holdout} forecasts (NIVELES)...\")\n",
    "        logger.info(f\"üìå Model is PRE-TRAINED (no retraining during forecast)\")\n",
    "        logger.info(f\"üìå Predicting LEVELS (PEN ‚âà 3.64), not returns\")\n",
    "        \n",
    "        for i in range(n_holdout):\n",
    "            if i % 10 == 0:\n",
    "                logger.info(f\"  Forecasting day {i+1}/{n_holdout}...\")\n",
    "            \n",
    "            # Contexto: train + holdout hasta i (valores reales como INPUT)\n",
    "            context_length = len(self.train_df) + i\n",
    "            context_df = self.full_df.iloc[:context_length].copy()\n",
    "            \n",
    "            try:\n",
    "                # Forecast con TimesFM (SOLO INFERENCE, NO RETRAINING)\n",
    "                # Ahora predice NIVELES\n",
    "                forecast_df = self.model.forecast_on_df(\n",
    "                    inputs=context_df,\n",
    "                    freq=\"D\",\n",
    "                    value_name=\"y\",\n",
    "                    num_jobs=-1,\n",
    "                    forecast_context_len=min(TIMESFM_CONFIG['context_len'], len(context_df))\n",
    "                )\n",
    "                \n",
    "                # Extraer predicci√≥n h=1 (NIVEL)\n",
    "                if len(forecast_df) > 0 and 'timesfm' in forecast_df.columns:\n",
    "                    pred_level = forecast_df['timesfm'].iloc[0]\n",
    "                    predictions_levels.append(float(pred_level))\n",
    "                else:\n",
    "                    logger.warning(f\"Empty or invalid forecast at step {i}\")\n",
    "                    # Usar √∫ltimo valor conocido como fallback\n",
    "                    predictions_levels.append(float(context_df['y'].iloc[-1]))\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error at step {i}: {str(e)}\")\n",
    "                # Usar √∫ltimo valor conocido como fallback\n",
    "                predictions_levels.append(float(context_df['y'].iloc[-1]))\n",
    "        \n",
    "        predictions_levels = np.array(predictions_levels)\n",
    "        \n",
    "        logger.info(f\"‚úì Forecasts generated: {len(predictions_levels)} predictions (NIVELES)\")\n",
    "        logger.info(f\"  Predictions range: [{predictions_levels.min():.4f}, {predictions_levels.max():.4f}]\")\n",
    "        logger.info(f\"  Predictions mean: {predictions_levels.mean():.4f}\")\n",
    "        logger.info(f\"  Predictions std: {predictions_levels.std():.4f}\")\n",
    "        \n",
    "        # üìå VALIDACI√ìN CR√çTICA\n",
    "        if len(predictions_levels) != N_HOLDOUT:\n",
    "            raise ValueError(f\"Expected {N_HOLDOUT} predictions, got {len(predictions_levels)}\")\n",
    "        \n",
    "        return {\n",
    "            'predictions_levels': predictions_levels,\n",
    "            'n_predictions': len(predictions_levels)\n",
    "        }\n",
    "\n",
    "logger.info(\"‚úì TimesFMEvaluator class defined (VERSI√ìN NIVELES)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "285ba4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:19:51 - INFO - ‚úì CheckpointManager initialized\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ================================================================================\n",
    "# CELDA 6: CheckpointManager CLASS\n",
    "# ================================================================================\n",
    "\n",
    "class CheckpointManager:\n",
    "    \"\"\"Sistema de checkpoints para recuperaci√≥n\"\"\"\n",
    "    \n",
    "    def __init__(self, checkpoint_dir: Path):\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    def save_checkpoint(self, name: str, data: Dict) -> None:\n",
    "        \"\"\"Guardar checkpoint\"\"\"\n",
    "        checkpoint_path = self.checkpoint_dir / f\"{name}.json\"\n",
    "        \n",
    "        # Convertir numpy arrays a listas para JSON\n",
    "        serializable_data = {}\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, np.ndarray):\n",
    "                serializable_data[key] = value.tolist()\n",
    "            else:\n",
    "                serializable_data[key] = value\n",
    "        \n",
    "        with open(checkpoint_path, 'w') as f:\n",
    "            json.dump(serializable_data, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"‚úì Checkpoint saved: {name}\")\n",
    "    \n",
    "    def load_checkpoint(self, name: str) -> Dict:\n",
    "        \"\"\"Cargar checkpoint\"\"\"\n",
    "        checkpoint_path = self.checkpoint_dir / f\"{name}.json\"\n",
    "        \n",
    "        if not checkpoint_path.exists():\n",
    "            raise FileNotFoundError(f\"Checkpoint not found: {name}\")\n",
    "        \n",
    "        with open(checkpoint_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        logger.info(f\"‚úì Checkpoint loaded: {name}\")\n",
    "        return data\n",
    "    \n",
    "    def checkpoint_exists(self, name: str) -> bool:\n",
    "        \"\"\"Verificar si checkpoint existe\"\"\"\n",
    "        checkpoint_path = self.checkpoint_dir / f\"{name}.json\"\n",
    "        return checkpoint_path.exists()\n",
    "\n",
    "checkpoint_manager = CheckpointManager(CHECKPOINT_DIR)\n",
    "logger.info(\"‚úì CheckpointManager initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15decc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:19:51 - INFO - ‚úì CheckpointManager initialized\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# CELDA 6: CheckpointManager CLASS\n",
    "# ================================================================================\n",
    "\n",
    "class CheckpointManager:\n",
    "    \"\"\"Sistema de checkpoints para recuperaci√≥n\"\"\"\n",
    "    \n",
    "    def __init__(self, checkpoint_dir: Path):\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    def save_checkpoint(self, name: str, data: Dict) -> None:\n",
    "        \"\"\"Guardar checkpoint\"\"\"\n",
    "        checkpoint_path = self.checkpoint_dir / f\"{name}.json\"\n",
    "        \n",
    "        # Convertir numpy arrays a listas para JSON\n",
    "        serializable_data = {}\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, np.ndarray):\n",
    "                serializable_data[key] = value.tolist()\n",
    "            else:\n",
    "                serializable_data[key] = value\n",
    "        \n",
    "        with open(checkpoint_path, 'w') as f:\n",
    "            json.dump(serializable_data, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"‚úì Checkpoint saved: {name}\")\n",
    "    \n",
    "    def load_checkpoint(self, name: str) -> Dict:\n",
    "        \"\"\"Cargar checkpoint\"\"\"\n",
    "        checkpoint_path = self.checkpoint_dir / f\"{name}.json\"\n",
    "        \n",
    "        if not checkpoint_path.exists():\n",
    "            raise FileNotFoundError(f\"Checkpoint not found: {name}\")\n",
    "        \n",
    "        with open(checkpoint_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        logger.info(f\"‚úì Checkpoint loaded: {name}\")\n",
    "        return data\n",
    "    \n",
    "    def checkpoint_exists(self, name: str) -> bool:\n",
    "        \"\"\"Verificar si checkpoint existe\"\"\"\n",
    "        checkpoint_path = self.checkpoint_dir / f\"{name}.json\"\n",
    "        return checkpoint_path.exists()\n",
    "\n",
    "checkpoint_manager = CheckpointManager(CHECKPOINT_DIR)\n",
    "logger.info(\"‚úì CheckpointManager initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5537b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:19:51 - INFO - ‚úì save_oof_predictions function defined\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ================================================================================\n",
    "# CELDA 7: FUNCI√ìN PARA GUARDAR OOF\n",
    "# ================================================================================\n",
    "\n",
    "def save_oof_predictions(predictions: np.ndarray,\n",
    "                         dates: List,\n",
    "                         actuals: np.ndarray,\n",
    "                         model_name: str,\n",
    "                         prediction_type: str,\n",
    "                         metadata: Dict,\n",
    "                         output_dir: Path) -> None:\n",
    "    \"\"\"\n",
    "    Guardar predicciones OOF en formato est√°ndar.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Array de predicciones (RETORNOS para meta-learner)\n",
    "        dates: Lista de fechas\n",
    "        actuals: Array de valores reales (RETORNOS)\n",
    "        model_name: Nombre del modelo\n",
    "        prediction_type: 'log_returns' o 'levels'\n",
    "        metadata: Diccionario con metadata adicional\n",
    "        output_dir: Directorio de salida\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Crear DataFrame\n",
    "    df_oof = pd.DataFrame({\n",
    "        'ds': dates,\n",
    "        'y_pred': predictions,\n",
    "        'y_real': actuals,\n",
    "        'model': model_name,\n",
    "        'type': prediction_type\n",
    "    })\n",
    "    \n",
    "    # Validaciones\n",
    "    n_zeros = (df_oof['y_pred'] == 0).sum()\n",
    "    \n",
    "    # Guardar CSV\n",
    "    csv_path = output_dir / f'train_oof_{model_name}.csv'\n",
    "    df_oof.to_csv(csv_path, index=False)\n",
    "    \n",
    "    logger.info(f\"‚úì OOF predictions saved: {csv_path}\")\n",
    "    logger.info(f\"  Observations: {len(df_oof)}\")\n",
    "    logger.info(f\"  Zeros: {n_zeros}\")\n",
    "    logger.info(f\"  Type: {prediction_type}\")\n",
    "    \n",
    "    # Guardar metadata\n",
    "    json_path = output_dir / f'train_oof_{model_name}_metadata.json'\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2, default=str)\n",
    "    \n",
    "    logger.info(f\"‚úì OOF metadata saved: {json_path}\")\n",
    "\n",
    "logger.info(\"‚úì save_oof_predictions function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "268c4ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:19:51 - INFO - ================================================================================\n",
      "2025-11-26 15:19:51 - INFO - LOADING DATA\n",
      "2025-11-26 15:19:51 - INFO - ================================================================================\n",
      "2025-11-26 15:19:51 - INFO - ‚úì Data loaded: 8201 rows, 12 columns\n",
      "2025-11-26 15:19:51 - INFO - Date range: 1994-01-31 00:00:00 to 2025-07-07 00:00:00\n",
      "2025-11-26 15:19:51 - INFO - Columns: ['PEN', 'MXN', 'CLP', 'COBRE', 'DXY', 'UST10Y', 'CPI', 'MXPE', 'RESERV', 'T_TRADE', 'Tasa_cds']\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ================================================================================\n",
    "# CELDA 8: CARGAR DATOS\n",
    "# ================================================================================\n",
    "\n",
    "logger.info(\"=\" * 80)\n",
    "logger.info(\"LOADING DATA\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "# Cargar CSV\n",
    "df_raw = pd.read_csv(DATA_PATH)\n",
    "logger.info(f\"‚úì Data loaded: {len(df_raw)} rows, {len(df_raw.columns)} columns\")\n",
    "\n",
    "# Convertir fecha a DatetimeIndex\n",
    "df_raw['Dates'] = pd.to_datetime(df_raw['Dates'])\n",
    "df_raw = df_raw.set_index('Dates')\n",
    "df_raw = df_raw.sort_index()\n",
    "\n",
    "logger.info(f\"Date range: {df_raw.index.min()} to {df_raw.index.max()}\")\n",
    "logger.info(f\"Columns: {list(df_raw.columns)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b19108fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:19:51 - INFO - ================================================================================\n",
      "2025-11-26 15:19:51 - INFO - DATA PREPARATION (NIVELES)\n",
      "2025-11-26 15:19:51 - INFO - ================================================================================\n",
      "2025-11-26 15:19:51 - INFO - ‚úì Target: PEN (NIVELES)\n",
      "2025-11-26 15:19:51 - INFO -    PEN range: [2.0520, 4.1375]\n",
      "2025-11-26 15:19:51 - INFO -    PEN mean: 3.1662\n",
      "2025-11-26 15:19:51 - INFO - Final dataset shape: (8200, 13)\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ================================================================================\n",
    "# CELDA 9: PREPARAR DATOS - ‚òÖ‚òÖ‚òÖ USAR NIVELES ‚òÖ‚òÖ‚òÖ\n",
    "# ================================================================================\n",
    "\n",
    "logger.info(\"=\" * 80)\n",
    "logger.info(\"DATA PREPARATION (NIVELES)\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "# TimesFM NO acepta ex√≥genas, solo necesitamos el target (NIVELES)\n",
    "df = df_raw.copy()\n",
    "\n",
    "# ‚òÖ‚òÖ‚òÖ NO creamos log returns como target - usamos NIVELES directamente ‚òÖ‚òÖ‚òÖ\n",
    "# Solo creamos log returns para referencia y c√°lculo de m√©tricas\n",
    "df['PEN_log_ret'] = np.log(df['PEN'] / df['PEN'].shift(1))\n",
    "\n",
    "# PEN_lag_1 para conversi√≥n a retornos despu√©s\n",
    "df['PEN_lag_1'] = df['PEN'].shift(1)\n",
    "\n",
    "# Forward fill y limpiar NaN\n",
    "df = df.ffill()\n",
    "df = df.dropna()\n",
    "\n",
    "logger.info(f\"‚úì Target: PEN (NIVELES)\")\n",
    "logger.info(f\"   PEN range: [{df['PEN'].min():.4f}, {df['PEN'].max():.4f}]\")\n",
    "logger.info(f\"   PEN mean: {df['PEN'].mean():.4f}\")\n",
    "logger.info(f\"Final dataset shape: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f078dd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:19:51 - INFO - ================================================================================\n",
      "2025-11-26 15:19:51 - INFO - TRAIN/HOLDOUT SPLIT\n",
      "2025-11-26 15:19:51 - INFO - ================================================================================\n",
      "2025-11-26 15:19:51 - INFO - Train: 8140 observations (99.3%)\n",
      "2025-11-26 15:19:51 - INFO -   Date range: 1994-02-01 00:00:00 to 2025-04-14 00:00:00\n",
      "2025-11-26 15:19:51 - INFO - Holdout: 60 observations (0.7%)\n",
      "2025-11-26 15:19:51 - INFO -   Date range: 2025-04-15 00:00:00 to 2025-07-07 00:00:00\n",
      "2025-11-26 15:19:51 - INFO - Train PEN levels: [2.0520, 4.1375]\n",
      "2025-11-26 15:19:51 - INFO - Holdout PEN levels: [3.5425, 3.7420]\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ================================================================================\n",
    "# CELDA 10: TRAIN/HOLDOUT SPLIT\n",
    "# ================================================================================\n",
    "\n",
    "logger.info(\"=\" * 80)\n",
    "logger.info(\"TRAIN/HOLDOUT SPLIT\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "# Split: √∫ltimos N_HOLDOUT d√≠as\n",
    "split_idx = len(df) - N_HOLDOUT\n",
    "\n",
    "train_df = df.iloc[:split_idx].copy()\n",
    "holdout_df = df.iloc[split_idx:].copy()\n",
    "\n",
    "logger.info(f\"Train: {len(train_df)} observations ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "logger.info(f\"  Date range: {train_df.index.min()} to {train_df.index.max()}\")\n",
    "logger.info(f\"Holdout: {len(holdout_df)} observations ({len(holdout_df)/len(df)*100:.1f}%)\")\n",
    "logger.info(f\"  Date range: {holdout_df.index.min()} to {holdout_df.index.max()}\")\n",
    "\n",
    "# Validaci√≥n cr√≠tica\n",
    "assert len(holdout_df) == N_HOLDOUT, f\"Holdout must have exactly {N_HOLDOUT} observations\"\n",
    "\n",
    "# Extraer niveles\n",
    "train_levels = train_df['PEN'].values\n",
    "holdout_levels = holdout_df['PEN'].values\n",
    "\n",
    "logger.info(f\"Train PEN levels: [{train_levels.min():.4f}, {train_levels.max():.4f}]\")\n",
    "logger.info(f\"Holdout PEN levels: [{holdout_levels.min():.4f}, {holdout_levels.max():.4f}]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd937706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:19:51 - INFO - ================================================================================\n",
      "2025-11-26 15:19:51 - INFO - CONVERT TO TIMESFM FORMAT (NIVELES)\n",
      "2025-11-26 15:19:51 - INFO - ================================================================================\n",
      "2025-11-26 15:19:51 - INFO - Converted to TimesFM format:\n",
      "2025-11-26 15:19:51 - INFO -   Rows: 8200\n",
      "2025-11-26 15:19:51 - INFO -   Value column: PEN (NIVELES)\n",
      "2025-11-26 15:19:51 - INFO -   Y range: [2.0520, 4.1375]\n",
      "2025-11-26 15:19:51 - INFO - Split data:\n",
      "2025-11-26 15:19:51 - INFO -   Train: 8140 observations\n",
      "2025-11-26 15:19:51 - INFO -   Holdout: 60 observations\n",
      "2025-11-26 15:19:51 - INFO - ‚úì Data converted to TimesFM format (NIVELES)\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ================================================================================\n",
    "# CELDA 11: CONVERTIR A FORMATO TIMESFM (NIVELES)\n",
    "# ================================================================================\n",
    "\n",
    "logger.info(\"=\" * 80)\n",
    "logger.info(\"CONVERT TO TIMESFM FORMAT (NIVELES)\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "converter = TimesFMConverter()\n",
    "\n",
    "# Convertir full dataset - USANDO NIVELES\n",
    "full_timesfm = converter.convert_to_timesfm_df(\n",
    "    df=df,\n",
    "    value_col=TARGET_COL,  # 'PEN' (niveles)\n",
    "    series_id='USDPEN',\n",
    "    freq='D'\n",
    ")\n",
    "\n",
    "# Split en formato TimesFM\n",
    "train_timesfm, holdout_timesfm = converter.split_timesfm_data(\n",
    "    timesfm_df=full_timesfm,\n",
    "    n_holdout=N_HOLDOUT\n",
    ")\n",
    "\n",
    "logger.info(\"‚úì Data converted to TimesFM format (NIVELES)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4453f89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:19:51 - INFO - ================================================================================\n",
      "2025-11-26 15:19:51 - INFO - PHASE 1: ZERO-SHOT EVALUATION (TimesFM - NIVELES)\n",
      "2025-11-26 15:19:51 - INFO - ================================================================================\n",
      "2025-11-26 15:19:51 - INFO - Starting zero-shot evaluation from scratch...\n",
      "2025-11-26 15:19:51 - INFO - Initializing TimesFM model...\n",
      "2025-11-26 15:19:51 - INFO - ‚ö†Ô∏è This may take a few minutes on first run (downloading model)...\n",
      "2025-11-26 15:19:52 - INFO - HTTP Request: GET https://huggingface.co/api/models/google/timesfm-1.0-200m-pytorch/revision/main \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01daa118b81f427bade14ba5b87bf07e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a256ef75ecf4ae2919b2c3d0f68a322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:19:54 - INFO - Loading checkpoint from C:\\Users\\Carlos Palma\\.cache\\huggingface\\hub\\models--google--timesfm-1.0-200m-pytorch\\snapshots\\0581e2c56cb06feb51cfd98fc2b4005b74f7187b\\torch_model.ckpt\n",
      "2025-11-26 15:19:54 - INFO - Sending checkpoint to device cuda:0\n",
      "2025-11-26 15:19:54 - INFO - ‚úì TimesFM initialized\n",
      "2025-11-26 15:19:54 - INFO -   Model: google/timesfm-1.0-200m-pytorch\n",
      "2025-11-26 15:19:54 - INFO -   Context length: 512\n",
      "2025-11-26 15:19:54 - INFO -   Horizon: h=1\n",
      "2025-11-26 15:19:54 - INFO -   Backend: gpu\n",
      "2025-11-26 15:19:54 - INFO - ================================================================================\n",
      "2025-11-26 15:19:54 - INFO - ROLLING FORECAST (TimesFM - NIVELES)\n",
      "2025-11-26 15:19:54 - INFO - ================================================================================\n",
      "2025-11-26 15:19:54 - INFO - Generating 60 forecasts (NIVELES)...\n",
      "2025-11-26 15:19:54 - INFO - üìå Model is PRE-TRAINED (no retraining during forecast)\n",
      "2025-11-26 15:19:54 - INFO - üìå Predicting LEVELS (PEN ‚âà 3.64), not returns\n",
      "2025-11-26 15:19:54 - INFO -   Forecasting day 1/60...\n",
      "2025-11-26 15:19:54 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:19:59 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:19:59 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:20:03 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:20:03 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:20:08 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:20:08 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:20:13 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:20:13 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:20:17 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:20:17 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:20:22 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:20:22 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:20:27 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:20:27 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:20:31 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:20:31 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:20:36 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:20:36 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:20:40 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:20:40 - INFO -   Forecasting day 11/60...\n",
      "2025-11-26 15:20:40 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:20:45 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:20:45 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:20:50 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:20:50 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:20:55 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:20:55 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:21:00 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:21:00 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:21:06 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:21:06 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:21:11 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:21:11 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:21:17 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:21:17 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:21:22 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:21:22 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:21:27 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:21:27 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:21:32 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:21:32 - INFO -   Forecasting day 21/60...\n",
      "2025-11-26 15:21:32 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:21:37 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:21:37 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:21:42 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:21:42 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:21:47 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:21:47 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:21:52 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:21:52 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:21:57 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:21:57 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:22:02 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:22:02 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preprocessing dataframe.\n",
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:22:07 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:22:07 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n",
      "Finished forecasting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:22:13 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:22:13 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:22:18 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:22:18 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:22:24 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:22:24 - INFO -   Forecasting day 31/60...\n",
      "2025-11-26 15:22:24 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:22:29 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:22:29 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:22:34 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:22:35 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:22:40 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:22:40 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:22:45 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:22:45 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:22:50 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:22:50 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:22:56 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:22:56 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:23:01 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:23:01 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:23:05 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:23:05 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:23:10 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:23:10 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:23:15 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:23:15 - INFO -   Forecasting day 41/60...\n",
      "2025-11-26 15:23:15 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:23:20 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:23:20 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:23:25 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:23:25 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:23:30 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:23:30 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:23:34 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:23:34 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:23:39 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:23:39 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:23:44 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:23:44 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:23:50 - INFO - Finished creating output dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preprocessing dataframe.\n",
      "Finished forecasting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:23:50 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:23:55 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:23:55 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:24:00 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:24:00 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:24:05 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:24:05 - INFO -   Forecasting day 51/60...\n",
      "2025-11-26 15:24:05 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:24:10 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:24:10 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:24:15 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:24:15 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:24:20 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:24:20 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n",
      "Finished forecasting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:24:25 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:24:25 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:24:30 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:24:30 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:24:35 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:24:35 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:24:40 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:24:40 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:24:45 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:24:45 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n",
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n",
      "Finished forecasting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:24:51 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:24:51 - INFO - Preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataframe with multiple processes.\n",
      "Finished preprocessing dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:24:56 - INFO - Finished creating output dataframe.\n",
      "2025-11-26 15:24:56 - INFO - ‚úì Forecasts generated: 60 predictions (NIVELES)\n",
      "2025-11-26 15:24:56 - INFO -   Predictions range: [3.5428, 3.7379]\n",
      "2025-11-26 15:24:56 - INFO -   Predictions mean: 3.6391\n",
      "2025-11-26 15:24:56 - INFO -   Predictions std: 0.0505\n",
      "2025-11-26 15:24:56 - INFO - ‚úì Checkpoint saved: zero_shot_results_niveles\n",
      "2025-11-26 15:24:56 - INFO - ‚úì Zero-shot evaluation completed and checkpointed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ================================================================================\n",
    "# CELDA 12: INICIALIZAR TIMESFM Y EJECUTAR ZERO-SHOT\n",
    "# ================================================================================\n",
    "\n",
    "logger.info(\"=\" * 80)\n",
    "logger.info(\"PHASE 1: ZERO-SHOT EVALUATION (TimesFM - NIVELES)\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "# Verificar checkpoint existente\n",
    "if checkpoint_manager.checkpoint_exists('zero_shot_results_niveles'):\n",
    "    logger.info(\"‚ö†Ô∏è Zero-shot checkpoint exists. Loading...\")\n",
    "    zero_shot_results = checkpoint_manager.load_checkpoint('zero_shot_results_niveles')\n",
    "    predictions_levels = np.array(zero_shot_results['predictions_levels'])\n",
    "else:\n",
    "    logger.info(\"Starting zero-shot evaluation from scratch...\")\n",
    "    \n",
    "    # Inicializar TimesFM\n",
    "    logger.info(\"Initializing TimesFM model...\")\n",
    "    logger.info(\"‚ö†Ô∏è This may take a few minutes on first run (downloading model)...\")\n",
    "    \n",
    "    model = timesfm.TimesFm(\n",
    "        hparams=timesfm.TimesFmHparams(\n",
    "            context_len=TIMESFM_CONFIG['context_len'],\n",
    "            horizon_len=TIMESFM_CONFIG['horizon_len'],\n",
    "            per_core_batch_size=TIMESFM_CONFIG['batch_size'],\n",
    "            backend=TIMESFM_CONFIG['backend']\n",
    "        ),\n",
    "        checkpoint=timesfm.TimesFmCheckpoint(\n",
    "            huggingface_repo_id=TIMESFM_CONFIG['model_id']\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"‚úì TimesFM initialized\")\n",
    "    logger.info(f\"  Model: {TIMESFM_CONFIG['model_id']}\")\n",
    "    logger.info(f\"  Context length: {TIMESFM_CONFIG['context_len']}\")\n",
    "    logger.info(f\"  Horizon: h={TIMESFM_CONFIG['horizon_len']}\")\n",
    "    logger.info(f\"  Backend: {TIMESFM_CONFIG['backend']}\")\n",
    "    \n",
    "    # Crear evaluator\n",
    "    evaluator = TimesFMEvaluator(\n",
    "        model=model,\n",
    "        train_df=train_timesfm,\n",
    "        holdout_df=holdout_timesfm,\n",
    "        full_df=full_timesfm\n",
    "    )\n",
    "    \n",
    "    # Rolling forecast (NIVELES)\n",
    "    forecast_results = evaluator.rolling_forecast()\n",
    "    predictions_levels = forecast_results['predictions_levels']\n",
    "    \n",
    "    # Guardar checkpoint\n",
    "    zero_shot_results = {\n",
    "        'predictions_levels': predictions_levels.tolist()\n",
    "    }\n",
    "    checkpoint_manager.save_checkpoint('zero_shot_results_niveles', zero_shot_results)\n",
    "    \n",
    "    logger.info(\"‚úì Zero-shot evaluation completed and checkpointed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c845e460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:26:33 - INFO - ================================================================================\n",
      "2025-11-26 15:26:33 - INFO - CALCULATING METRICS (NIVELES ‚Üí RETORNOS)\n",
      "2025-11-26 15:26:33 - INFO - ================================================================================\n",
      "2025-11-26 15:26:33 - INFO - ‚úì Metrics calculated (en espacio de RETORNOS)\n",
      "2025-11-26 15:26:33 - INFO - \n",
      "2025-11-26 15:26:33 - INFO - ================================================================================\n",
      "2025-11-26 15:26:33 - INFO - RESULTS - TIMESFM ZERO-SHOT (NIVELES)\n",
      "2025-11-26 15:26:33 - INFO - ================================================================================\n",
      "2025-11-26 15:26:33 - INFO - \n",
      "2025-11-26 15:26:33 - INFO - üìä M√âTRICAS FINALES (calculadas en RETORNOS):\n",
      "2025-11-26 15:26:33 - INFO -    DA:   58.33%\n",
      "2025-11-26 15:26:33 - INFO -    MASE: 1.3022\n",
      "2025-11-26 15:26:33 - INFO -    MAE:  0.002530\n",
      "2025-11-26 15:26:33 - INFO - \n",
      "2025-11-26 15:26:33 - INFO - üìä COMPARACI√ìN CON BASELINE:\n",
      "2025-11-26 15:26:33 - INFO -    ARX (baseline): DA=51.67%, MASE=0.9398\n",
      "2025-11-26 15:26:33 - INFO -    TimesFM:        DA=58.33%, MASE=1.3022\n",
      "2025-11-26 15:26:33 - INFO -    Delta DA:       +6.66%\n",
      "2025-11-26 15:26:33 - INFO -    Delta MASE:     +0.3624\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ================================================================================\n",
    "# CELDA 13: CALCULAR M√âTRICAS - ‚òÖ‚òÖ‚òÖ CONVERSI√ìN NIVELES‚ÜíRETORNOS ‚òÖ‚òÖ‚òÖ\n",
    "# ================================================================================\n",
    "\n",
    "logger.info(\"=\" * 80)\n",
    "logger.info(\"CALCULATING METRICS (NIVELES ‚Üí RETORNOS)\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# ‚òÖ‚òÖ‚òÖ CONVERSI√ìN A RETORNOS PARA M√âTRICAS COMPARABLES ‚òÖ‚òÖ‚òÖ\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "# Niveles reales y anteriores del holdout\n",
    "y_true_levels = holdout_df['PEN'].values\n",
    "y_prev_levels = holdout_df['PEN_lag_1'].values\n",
    "\n",
    "# Calcular m√©tricas usando conversi√≥n niveles‚Üíretornos\n",
    "da = FXMetrics.directional_accuracy_from_levels(\n",
    "    y_true_levels, predictions_levels, y_prev_levels\n",
    ")\n",
    "mase = FXMetrics.mase_from_levels(\n",
    "    y_true_levels, predictions_levels, y_prev_levels, train_levels\n",
    ")\n",
    "mae = FXMetrics.mae_from_levels(\n",
    "    y_true_levels, predictions_levels, y_prev_levels\n",
    ")\n",
    "\n",
    "# Convertir predicciones de niveles a retornos (para exportaci√≥n)\n",
    "predictions_returns = np.log(predictions_levels / y_prev_levels)\n",
    "y_true_returns = np.log(y_true_levels / y_prev_levels)\n",
    "\n",
    "logger.info(\"‚úì Metrics calculated (en espacio de RETORNOS)\")\n",
    "logger.info(\"\")\n",
    "logger.info(\"=\" * 80)\n",
    "logger.info(\"RESULTS - TIMESFM ZERO-SHOT (NIVELES)\")\n",
    "logger.info(\"=\" * 80)\n",
    "logger.info(f\"\")\n",
    "logger.info(f\"üìä M√âTRICAS FINALES (calculadas en RETORNOS):\")\n",
    "logger.info(f\"   DA:   {da:.2f}%\")\n",
    "logger.info(f\"   MASE: {mase:.4f}\")\n",
    "logger.info(f\"   MAE:  {mae:.6f}\")\n",
    "logger.info(f\"\")\n",
    "logger.info(f\"üìä COMPARACI√ìN CON BASELINE:\")\n",
    "logger.info(f\"   ARX (baseline): DA={BASELINE_ARX['DA']:.2f}%, MASE={BASELINE_ARX['MASE']:.4f}\")\n",
    "logger.info(f\"   TimesFM:        DA={da:.2f}%, MASE={mase:.4f}\")\n",
    "logger.info(f\"   Delta DA:       {da - BASELINE_ARX['DA']:+.2f}%\")\n",
    "logger.info(f\"   Delta MASE:     {mase - BASELINE_ARX['MASE']:+.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a08af480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:26:51 - INFO - ‚úì generate_oof_predictions_timesfm function defined (VERSI√ìN NIVELES)\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ================================================================================\n",
    "# CELDA 13.5: GENERATE OOF PREDICTIONS - ‚òÖ‚òÖ‚òÖ VERSI√ìN NIVELES ‚òÖ‚òÖ‚òÖ\n",
    "# ================================================================================\n",
    "\n",
    "def generate_oof_predictions_timesfm(model,\n",
    "                                      train_df_timesfm: pd.DataFrame,\n",
    "                                      full_df_timesfm: pd.DataFrame,\n",
    "                                      train_df_original: pd.DataFrame,\n",
    "                                      checkpoint_dir: Path = None) -> dict:\n",
    "    \"\"\"\n",
    "    Generar predicciones Out-of-Fold usando Walk-Forward CV.\n",
    "    \n",
    "    El modelo predice NIVELES, luego convertimos a RETORNOS para el meta-learner.\n",
    "    \n",
    "    Args:\n",
    "        model: TimesFM model instance (pre-trained)\n",
    "        train_df_timesfm: Train data en formato TimesFM\n",
    "        full_df_timesfm: Full data (train + holdout) en formato TimesFM\n",
    "        train_df_original: DataFrame original con PEN_lag_1\n",
    "        checkpoint_dir: Directorio para checkpoints\n",
    "    \n",
    "    Returns:\n",
    "        dict con predicciones OOF (en RETORNOS), m√©tricas y metadata\n",
    "    \"\"\"\n",
    "    \n",
    "    if checkpoint_dir is None:\n",
    "        checkpoint_dir = OOF_DIR\n",
    "    \n",
    "    checkpoint_dir = Path(checkpoint_dir)\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Archivo de checkpoint temporal\n",
    "    checkpoint_file = checkpoint_dir / 'oof_timesfm_niveles_checkpoint.pkl'\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üîÑ GENERANDO OOF PREDICTIONS (NIVELES ‚Üí RETORNOS)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"üìå TimesFM es pre-entrenado: NO reentrenamos, solo forecasting\")\n",
    "    print(\"üìå Predice NIVELES, convertimos a RETORNOS para meta-learner\")\n",
    "    \n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    # PASO 1: Verificar si existe checkpoint\n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    \n",
    "    resume_from_checkpoint = False\n",
    "    checkpoint_data = None\n",
    "    \n",
    "    if checkpoint_file.exists():\n",
    "        print(f\"\\nüìÇ CHECKPOINT ENCONTRADO: {checkpoint_file}\")\n",
    "        print(\"   Continuando desde checkpoint...\")\n",
    "        \n",
    "        try:\n",
    "            with open(checkpoint_file, 'rb') as f:\n",
    "                checkpoint_data = pickle.load(f)\n",
    "            \n",
    "            print(f\"\\n‚úÖ Checkpoint cargado:\")\n",
    "            print(f\"   √öltimo fold completado: {checkpoint_data['last_fold']}\")\n",
    "            print(f\"   Predicciones guardadas: {len(checkpoint_data['oof_predictions'])}\")\n",
    "            \n",
    "            resume_from_checkpoint = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è Error cargando checkpoint: {e}\")\n",
    "            print(\"   Empezando desde cero...\")\n",
    "            resume_from_checkpoint = False\n",
    "    \n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    # PASO 2: Inicializar o recuperar estado\n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    \n",
    "    if resume_from_checkpoint and checkpoint_data:\n",
    "        oof_predictions = checkpoint_data['oof_predictions']\n",
    "        oof_dates = checkpoint_data['oof_dates']\n",
    "        oof_actuals = checkpoint_data['oof_actuals']\n",
    "        fold_metrics = checkpoint_data['fold_metrics']\n",
    "        start_fold = checkpoint_data['last_fold'] + 1\n",
    "        \n",
    "        print(f\"\\n‚ñ∂Ô∏è RESUMIENDO desde fold {start_fold}\")\n",
    "        \n",
    "    else:\n",
    "        oof_predictions = []  # Retornos\n",
    "        oof_dates = []\n",
    "        oof_actuals = []      # Retornos\n",
    "        fold_metrics = []\n",
    "        start_fold = 1\n",
    "        \n",
    "        print(f\"\\n‚ñ∂Ô∏è INICIANDO desde fold 1\")\n",
    "    \n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    # PASO 3: Configuraci√≥n Walk-Forward\n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    \n",
    "    n_train = len(train_df_timesfm)\n",
    "    n_splits = (n_train - MIN_TRAIN) // STEP_SIZE\n",
    "    \n",
    "    print(f\"\\nüìä Configuraci√≥n Walk-Forward:\")\n",
    "    print(f\"   Train TimesFM: {n_train}\")\n",
    "    print(f\"   MIN_TRAIN: {MIN_TRAIN}\")\n",
    "    print(f\"   STEP_SIZE: {STEP_SIZE}\")\n",
    "    print(f\"   N_SPLITS: {n_splits}\")\n",
    "    \n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    # PASO 4: Walk-Forward Loop\n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    \n",
    "    for fold_idx in range(start_fold - 1, n_splits):\n",
    "        fold_start_time = time.time()\n",
    "        \n",
    "        # Definir ventanas\n",
    "        train_end = MIN_TRAIN + fold_idx * STEP_SIZE\n",
    "        val_start = train_end\n",
    "        val_end = min(val_start + STEP_SIZE, n_train)\n",
    "        \n",
    "        if val_end > n_train:\n",
    "            break\n",
    "        \n",
    "        print(f\"\\n   Fold {fold_idx + 1}/{n_splits}: train[:{train_end}], val[{val_start}:{val_end}]\")\n",
    "        \n",
    "        fold_preds = []\n",
    "        fold_actuals = []\n",
    "        fold_dates = []\n",
    "        \n",
    "        # Rolling forecast dentro del fold\n",
    "        for i in range(val_start, val_end):\n",
    "            # Contexto: todo hasta i\n",
    "            context_df = train_df_timesfm.iloc[:i].copy()\n",
    "            \n",
    "            try:\n",
    "                # Forecast (NIVEL)\n",
    "                forecast_df = model.forecast_on_df(\n",
    "                    inputs=context_df,\n",
    "                    freq=\"D\",\n",
    "                    value_name=\"y\",\n",
    "                    num_jobs=-1,\n",
    "                    forecast_context_len=min(TIMESFM_CONFIG['context_len'], len(context_df))\n",
    "                )\n",
    "                \n",
    "                if len(forecast_df) > 0 and 'timesfm' in forecast_df.columns:\n",
    "                    pred_level = float(forecast_df['timesfm'].iloc[0])\n",
    "                else:\n",
    "                    pred_level = float(context_df['y'].iloc[-1])\n",
    "                    \n",
    "            except Exception as e:\n",
    "                pred_level = float(context_df['y'].iloc[-1])\n",
    "            \n",
    "            # Valores reales\n",
    "            true_level = float(train_df_timesfm.iloc[i]['y'])\n",
    "            prev_level = float(train_df_timesfm.iloc[i-1]['y'])\n",
    "            date = train_df_original.index[i]\n",
    "            \n",
    "            # ‚òÖ‚òÖ‚òÖ Convertir a RETORNOS para meta-learner ‚òÖ‚òÖ‚òÖ\n",
    "            pred_return = np.log(pred_level / prev_level)\n",
    "            true_return = np.log(true_level / prev_level)\n",
    "            \n",
    "            fold_preds.append(pred_return)\n",
    "            fold_actuals.append(true_return)\n",
    "            fold_dates.append(date)\n",
    "        \n",
    "        # Agregar a resultados globales\n",
    "        oof_predictions.extend(fold_preds)\n",
    "        oof_actuals.extend(fold_actuals)\n",
    "        oof_dates.extend(fold_dates)\n",
    "        \n",
    "        # M√©tricas del fold\n",
    "        fold_da = np.mean(np.sign(fold_preds) == np.sign(fold_actuals)) * 100\n",
    "        fold_mae = np.mean(np.abs(np.array(fold_preds) - np.array(fold_actuals)))\n",
    "        \n",
    "        fold_metrics.append({\n",
    "            'fold': fold_idx + 1,\n",
    "            'train_end': train_end,\n",
    "            'val_size': len(fold_preds),\n",
    "            'da': fold_da,\n",
    "            'mae': fold_mae\n",
    "        })\n",
    "        \n",
    "        fold_time = time.time() - fold_start_time\n",
    "        print(f\"      ‚úì {len(fold_preds)} preds | DA: {fold_da:.1f}% | Time: {fold_time:.1f}s\")\n",
    "        \n",
    "        # Guardar checkpoint cada 5 folds\n",
    "        if (fold_idx + 1) % 5 == 0:\n",
    "            checkpoint_data = {\n",
    "                'last_fold': fold_idx + 1,\n",
    "                'oof_predictions': oof_predictions,\n",
    "                'oof_dates': [str(d) for d in oof_dates],\n",
    "                'oof_actuals': oof_actuals,\n",
    "                'fold_metrics': fold_metrics,\n",
    "                'elapsed_seconds': time.time() - start_time\n",
    "            }\n",
    "            with open(checkpoint_file, 'wb') as f:\n",
    "                pickle.dump(checkpoint_data, f)\n",
    "            print(f\"      üíæ Checkpoint guardado (fold {fold_idx + 1})\")\n",
    "    \n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    # PASO 5: Calcular m√©tricas finales OOF\n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    \n",
    "    oof_predictions = np.array(oof_predictions)\n",
    "    oof_actuals = np.array(oof_actuals)\n",
    "    \n",
    "    # M√©tricas\n",
    "    da_oof = np.mean(np.sign(oof_predictions) == np.sign(oof_actuals)) * 100\n",
    "    mae_oof = np.mean(np.abs(oof_predictions - oof_actuals))\n",
    "    \n",
    "    # MASE\n",
    "    train_returns = np.log(train_levels[1:] / train_levels[:-1])\n",
    "    naive_mae = np.mean(np.abs(train_returns))\n",
    "    mase_oof = mae_oof / naive_mae if naive_mae > 0 else np.inf\n",
    "    \n",
    "    elapsed_total = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìä RESUMEN OOF FINAL\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"   Observaciones: {len(oof_predictions)}\")\n",
    "    print(f\"   DA OOF:        {da_oof:.2f}%\")\n",
    "    print(f\"   MAE OOF:       {mae_oof:.6f}\")\n",
    "    print(f\"   MASE OOF:      {mase_oof:.4f}\")\n",
    "    print(f\"   Tiempo total:  {elapsed_total/60:.1f} minutos\")\n",
    "    \n",
    "    # Guardar OOF definitivo\n",
    "    save_oof_predictions(\n",
    "        predictions=oof_predictions,\n",
    "        dates=oof_dates,\n",
    "        actuals=oof_actuals,\n",
    "        model_name='TimesFM',\n",
    "        prediction_type='log_returns',  # ‚Üê Exportamos RETORNOS\n",
    "        metadata={\n",
    "            'method': 'walk_forward_niveles',\n",
    "            'target_training': 'PEN (niveles)',\n",
    "            'output_format': 'log_returns',\n",
    "            'min_train': MIN_TRAIN,\n",
    "            'step_size': STEP_SIZE,\n",
    "            'n_splits': len(fold_metrics),\n",
    "            'n_observations': len(oof_predictions),\n",
    "            'model': 'TimesFM',\n",
    "            'model_id': TIMESFM_CONFIG['model_id'],\n",
    "            'da_oof': float(da_oof),\n",
    "            'mae_oof': float(mae_oof),\n",
    "            'mase_oof': float(mase_oof),\n",
    "            'elapsed_minutes': float(elapsed_total/60),\n",
    "            'note': 'Model trained on LEVELS, output converted to LOG_RETURNS'\n",
    "        },\n",
    "        output_dir=OOF_DIR\n",
    "    )\n",
    "    \n",
    "    # Eliminar checkpoint temporal\n",
    "    if checkpoint_file.exists():\n",
    "        checkpoint_file.unlink()\n",
    "        print(f\"\\nüóëÔ∏è Checkpoint temporal eliminado (proceso completado)\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ OOF completado exitosamente\")\n",
    "    print(f\"   üìÅ Guardado en: {OOF_DIR}/train_oof_TimesFM.csv\")\n",
    "    \n",
    "    return {\n",
    "        'predictions': oof_predictions,\n",
    "        'dates': oof_dates,\n",
    "        'actuals': oof_actuals,\n",
    "        'metrics': {\n",
    "            'da': da_oof,\n",
    "            'mae': mae_oof,\n",
    "            'mase': mase_oof\n",
    "        },\n",
    "        'fold_metrics': fold_metrics\n",
    "    }\n",
    "\n",
    "logger.info(\"‚úì generate_oof_predictions_timesfm function defined (VERSI√ìN NIVELES)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cefb87ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:26:58 - INFO - ================================================================================\n",
      "2025-11-26 15:26:58 - INFO - GENERATING OOF PREDICTIONS\n",
      "2025-11-26 15:26:58 - INFO - ================================================================================\n",
      "2025-11-26 15:26:58 - INFO - ‚ö†Ô∏è OOF file already exists: C:\\Users\\Carlos Palma\\OneDrive\\Documents\\Cursos\\UTEC Computer Science\\TESIS\\NUEVO PAPER\\tesis_maestria\\oof_predictions\\train_oof_TimesFM.csv\n",
      "2025-11-26 15:26:58 - INFO - ‚úì Usando OOF existente\n",
      "2025-11-26 15:26:58 - INFO - ‚úì OOF predictions ready\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ================================================================================\n",
    "# CELDA 14: GENERAR OOF\n",
    "# ================================================================================\n",
    "\n",
    "logger.info(\"=\" * 80)\n",
    "logger.info(\"GENERATING OOF PREDICTIONS\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "# Verificar si ya existe\n",
    "oof_csv_path = OOF_DIR / 'train_oof_TimesFM.csv'\n",
    "\n",
    "if oof_csv_path.exists():\n",
    "    logger.info(f\"‚ö†Ô∏è OOF file already exists: {oof_csv_path}\")\n",
    "    logger.info(\"‚úì Usando OOF existente\")\n",
    "    oof_df = pd.read_csv(oof_csv_path)\n",
    "    oof_results = {\n",
    "        'predictions': oof_df['y_pred'].values,\n",
    "        'dates': oof_df['ds'].tolist(),\n",
    "        'actuals': oof_df['y_real'].values\n",
    "    }\n",
    "else:\n",
    "    logger.info(\"üîÑ Generando OOF predictions...\")\n",
    "    oof_results = generate_oof_predictions_timesfm(\n",
    "        model=model,\n",
    "        train_df_timesfm=train_timesfm,\n",
    "        full_df_timesfm=full_timesfm,\n",
    "        train_df_original=train_df,\n",
    "        checkpoint_dir=OOF_DIR\n",
    "    )\n",
    "\n",
    "logger.info(\"‚úì OOF predictions ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7880d677",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:27:03 - INFO - \n",
      "2025-11-26 15:27:03 - INFO - ================================================================================\n",
      "2025-11-26 15:27:03 - INFO - üìå EXPORTING PREDICTIONS FOR META-LEARNER\n",
      "2025-11-26 15:27:03 - INFO - ================================================================================\n",
      "2025-11-26 15:27:03 - INFO - ‚úì Predictions exported: C:\\Users\\Carlos Palma\\OneDrive\\Documents\\Cursos\\UTEC Computer Science\\TESIS\\NUEVO PAPER\\tesis_maestria\\predictions_dump\\pred_TimesFM.csv\n",
      "2025-11-26 15:27:03 - INFO -   Rows: 60\n",
      "2025-11-26 15:27:03 - INFO -   Columns: ['ds', 'y_pred', 'model', 'type']\n",
      "2025-11-26 15:27:03 - INFO -   Prediction type: log_returns (converted from levels)\n",
      "2025-11-26 15:27:03 - INFO -   Date range: 2025-04-15 00:00:00 to 2025-07-07 00:00:00\n",
      "2025-11-26 15:27:03 - INFO - \n",
      "2025-11-26 15:27:03 - INFO - Preview of exported predictions:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ds    y_pred    model         type\n",
      "0 2025-04-15 -0.001533  TimesFM  log_returns\n",
      "1 2025-04-16 -0.001107  TimesFM  log_returns\n",
      "2 2025-04-17  0.000593  TimesFM  log_returns\n",
      "3 2025-04-18  0.000825  TimesFM  log_returns\n",
      "4 2025-04-21  0.000411  TimesFM  log_returns\n",
      "5 2025-04-22 -0.000475  TimesFM  log_returns\n",
      "6 2025-04-23 -0.001458  TimesFM  log_returns\n",
      "7 2025-04-24 -0.001190  TimesFM  log_returns\n",
      "8 2025-04-25 -0.000865  TimesFM  log_returns\n",
      "9 2025-04-28 -0.001470  TimesFM  log_returns\n",
      "...\n",
      "           ds    y_pred    model         type\n",
      "55 2025-07-01  0.000098  TimesFM  log_returns\n",
      "56 2025-07-02 -0.000115  TimesFM  log_returns\n",
      "57 2025-07-03  0.001034  TimesFM  log_returns\n",
      "58 2025-07-04  0.000417  TimesFM  log_returns\n",
      "59 2025-07-07  0.000303  TimesFM  log_returns\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ================================================================================\n",
    "# CELDA 15: üìå EXPORTAR PREDICCIONES PARA META-LEARNER (CR√çTICO)\n",
    "# ================================================================================\n",
    "\n",
    "logger.info(\"\")\n",
    "logger.info(\"=\" * 80)\n",
    "logger.info(\"üìå EXPORTING PREDICTIONS FOR META-LEARNER\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "# ‚òÖ‚òÖ‚òÖ Crear DataFrame con RETORNOS (convertidos desde niveles) ‚òÖ‚òÖ‚òÖ\n",
    "export_df = pd.DataFrame({\n",
    "    'ds': holdout_df.index,             # 60 fechas del holdout\n",
    "    'y_pred': predictions_returns,       # 60 predicciones de log returns (convertidas)\n",
    "    'model': 'TimesFM',                  # Nombre del modelo\n",
    "    'type': 'log_returns'                # Tipo de predicci√≥n\n",
    "})\n",
    "\n",
    "# Validaciones CR√çTICAS\n",
    "assert len(export_df) == N_HOLDOUT, f\"Expected {N_HOLDOUT} rows, got {len(export_df)}\"\n",
    "assert export_df['y_pred'].isna().sum() == 0, \"NaN values found in predictions\"\n",
    "assert export_df['model'].unique()[0] == 'TimesFM', \"Model name mismatch\"\n",
    "assert export_df['type'].unique()[0] == 'log_returns', \"Type mismatch\"\n",
    "\n",
    "# Guardar en predictions_dump/\n",
    "export_path = PREDICTIONS_DUMP / \"pred_TimesFM.csv\"\n",
    "export_df.to_csv(export_path, index=False)\n",
    "\n",
    "logger.info(f\"‚úì Predictions exported: {export_path}\")\n",
    "logger.info(f\"  Rows: {len(export_df)}\")\n",
    "logger.info(f\"  Columns: {list(export_df.columns)}\")\n",
    "logger.info(f\"  Prediction type: log_returns (converted from levels)\")\n",
    "logger.info(f\"  Date range: {export_df['ds'].min()} to {export_df['ds'].max()}\")\n",
    "\n",
    "# Vista previa\n",
    "logger.info(\"\")\n",
    "logger.info(\"Preview of exported predictions:\")\n",
    "print(export_df.head(10))\n",
    "print(\"...\")\n",
    "print(export_df.tail(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad8bddd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:27:19 - INFO - \n",
      "2025-11-26 15:27:19 - INFO - ================================================================================\n",
      "2025-11-26 15:27:19 - INFO - SAVING CONFIGURATION AND METRICS\n",
      "2025-11-26 15:27:19 - INFO - ================================================================================\n",
      "2025-11-26 15:27:19 - INFO - ‚úì Config saved: C:\\Users\\Carlos Palma\\OneDrive\\Documents\\Cursos\\UTEC Computer Science\\TESIS\\NUEVO PAPER\\tesis_maestria\\TimesFM_h1_USD_PEN\\config.json\n",
      "2025-11-26 15:27:19 - INFO - ‚úì Metrics saved: C:\\Users\\Carlos Palma\\OneDrive\\Documents\\Cursos\\UTEC Computer Science\\TESIS\\NUEVO PAPER\\tesis_maestria\\TimesFM_h1_USD_PEN\\metrics.json\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ================================================================================\n",
    "# CELDA 16: GUARDAR CONFIGURACI√ìN Y M√âTRICAS\n",
    "# ================================================================================\n",
    "\n",
    "logger.info(\"\")\n",
    "logger.info(\"=\" * 80)\n",
    "logger.info(\"SAVING CONFIGURATION AND METRICS\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "# Configuraci√≥n\n",
    "config = {\n",
    "    'model_name': 'TimesFM',\n",
    "    'model_id': TIMESFM_CONFIG['model_id'],\n",
    "    'variant': 'zero-shot',\n",
    "    'version': 'NIVELES',\n",
    "    'target': TARGET_COL,                   # 'PEN' (niveles)\n",
    "    'target_description': 'PEN levels, converted to log_returns for metrics',\n",
    "    'n_holdout': N_HOLDOUT,\n",
    "    'h_forecast': H_FORECAST,\n",
    "    'context_len': TIMESFM_CONFIG['context_len'],\n",
    "    'backend': TIMESFM_CONFIG['backend'],\n",
    "    'train_size': len(train_timesfm),\n",
    "    'holdout_size': len(holdout_timesfm),\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'exogenous_features': 0,                # TimesFM no usa ex√≥genas\n",
    "    'exogenous_list': [],\n",
    "    'methodology': 'rolling_forecast_no_retrain',\n",
    "    'run_id': RUN_ID,\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "config_path = OUTPUT_DIR / \"config.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "logger.info(f\"‚úì Config saved: {config_path}\")\n",
    "\n",
    "# M√©tricas\n",
    "metrics = {\n",
    "    'model': 'TimesFM',\n",
    "    'variant': 'zero-shot',\n",
    "    'version': 'NIVELES',\n",
    "    'DA': round(da, 2),\n",
    "    'MASE': round(mase, 4),\n",
    "    'MAE': round(mae, 6),\n",
    "    'n_predictions': len(predictions_levels),\n",
    "    'baseline_comparison': {\n",
    "        'ARX_DA': BASELINE_ARX['DA'],\n",
    "        'ARX_MASE': BASELINE_ARX['MASE'],\n",
    "        'delta_DA': round(da - BASELINE_ARX['DA'], 2),\n",
    "        'delta_MASE': round(mase - BASELINE_ARX['MASE'], 4)\n",
    "    },\n",
    "    'run_id': RUN_ID,\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "metrics_path = OUTPUT_DIR / \"metrics.json\"\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "logger.info(f\"‚úì Metrics saved: {metrics_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "abd8e89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 15:27:23 - INFO - \n",
      "2025-11-26 15:27:23 - INFO - ================================================================================\n",
      "2025-11-26 15:27:23 - INFO - üèÅ TIMESFM USD/PEN - COMPLETADO (VERSI√ìN NIVELES)\n",
      "2025-11-26 15:27:23 - INFO - ================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
      "‚ïë  TIMESFM USD/PEN FORECASTING - RESUMEN FINAL (VERSI√ìN NIVELES)               ‚ïë\n",
      "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
      "‚ïë                                                                              ‚ïë\n",
      "‚ïë  üìä CONFIGURACI√ìN:                                                           ‚ïë\n",
      "‚ïë     Target:          PEN (NIVELES)                                           ‚ïë\n",
      "‚ïë     Ex√≥genas:        Ninguna (TimesFM es univariado)                         ‚ïë\n",
      "‚ïë     Holdout:         60 d√≠as                                                ‚ïë\n",
      "‚ïë     Context length:  512                                              ‚ïë\n",
      "‚ïë                                                                              ‚ïë\n",
      "‚ïë  üìà M√âTRICAS HOLDOUT (calculadas en RETORNOS):                               ‚ïë\n",
      "‚ïë     DA:              58.33%                                                    ‚ïë\n",
      "‚ïë     MASE:            1.3022                                                   ‚ïë\n",
      "‚ïë     MAE:             0.002530                                                 ‚ïë\n",
      "‚ïë                                                                              ‚ïë\n",
      "‚ïë  üìä COMPARACI√ìN CON BASELINE:                                                ‚ïë\n",
      "‚ïë     ARX:             DA=51.67%, MASE=0.9398                          ‚ïë\n",
      "‚ïë     TimesFM:         DA=58.33%, MASE=1.3022                              ‚ïë\n",
      "‚ïë     Delta:           DA +6.66%, MASE +0.3624                    ‚ïë\n",
      "‚ïë                                                                              ‚ïë\n",
      "‚ïë  üìÅ ARCHIVOS GENERADOS:                                                      ‚ïë\n",
      "‚ïë     Predicciones:    C:\\Users\\Carlos Palma\\OneDrive\\Documents\\Cursos\\UTEC Computer Science\\TESIS\\NUEVO PAPER\\tesis_maestria\\predictions_dump/pred_TimesFM.csv            ‚ïë\n",
      "‚ïë     OOF:             C:\\Users\\Carlos Palma\\OneDrive\\Documents\\Cursos\\UTEC Computer Science\\TESIS\\NUEVO PAPER\\tesis_maestria\\oof_predictions/train_oof_TimesFM.csv                ‚ïë\n",
      "‚ïë     Config:          C:\\Users\\Carlos Palma\\OneDrive\\Documents\\Cursos\\UTEC Computer Science\\TESIS\\NUEVO PAPER\\tesis_maestria\\TimesFM_h1_USD_PEN/config.json                       ‚ïë\n",
      "‚ïë     Metrics:         C:\\Users\\Carlos Palma\\OneDrive\\Documents\\Cursos\\UTEC Computer Science\\TESIS\\NUEVO PAPER\\tesis_maestria\\TimesFM_h1_USD_PEN/metrics.json                      ‚ïë\n",
      "‚ïë                                                                              ‚ïë\n",
      "‚ïë  ‚úÖ Listo para meta-learner                                                  ‚ïë\n",
      "‚ïë                                                                              ‚ïë\n",
      "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ================================================================================\n",
    "# CELDA 17: RESUMEN FINAL\n",
    "# ================================================================================\n",
    "\n",
    "logger.info(\"\")\n",
    "logger.info(\"=\" * 80)\n",
    "logger.info(\"üèÅ TIMESFM USD/PEN - COMPLETADO (VERSI√ìN NIVELES)\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "print(f\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë  TIMESFM USD/PEN FORECASTING - RESUMEN FINAL (VERSI√ìN NIVELES)               ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë                                                                              ‚ïë\n",
    "‚ïë  üìä CONFIGURACI√ìN:                                                           ‚ïë\n",
    "‚ïë     Target:          PEN (NIVELES)                                           ‚ïë\n",
    "‚ïë     Ex√≥genas:        Ninguna (TimesFM es univariado)                         ‚ïë\n",
    "‚ïë     Holdout:         {N_HOLDOUT} d√≠as                                                ‚ïë\n",
    "‚ïë     Context length:  {TIMESFM_CONFIG['context_len']}                                              ‚ïë\n",
    "‚ïë                                                                              ‚ïë\n",
    "‚ïë  üìà M√âTRICAS HOLDOUT (calculadas en RETORNOS):                               ‚ïë\n",
    "‚ïë     DA:              {da:.2f}%                                                    ‚ïë\n",
    "‚ïë     MASE:            {mase:.4f}                                                   ‚ïë\n",
    "‚ïë     MAE:             {mae:.6f}                                                 ‚ïë\n",
    "‚ïë                                                                              ‚ïë\n",
    "‚ïë  üìä COMPARACI√ìN CON BASELINE:                                                ‚ïë\n",
    "‚ïë     ARX:             DA={BASELINE_ARX['DA']:.2f}%, MASE={BASELINE_ARX['MASE']:.4f}                          ‚ïë\n",
    "‚ïë     TimesFM:         DA={da:.2f}%, MASE={mase:.4f}                              ‚ïë\n",
    "‚ïë     Delta:           DA {da - BASELINE_ARX['DA']:+.2f}%, MASE {mase - BASELINE_ARX['MASE']:+.4f}                    ‚ïë\n",
    "‚ïë                                                                              ‚ïë\n",
    "‚ïë  üìÅ ARCHIVOS GENERADOS:                                                      ‚ïë\n",
    "‚ïë     Predicciones:    {PREDICTIONS_DUMP}/pred_TimesFM.csv            ‚ïë\n",
    "‚ïë     OOF:             {OOF_DIR}/train_oof_TimesFM.csv                ‚ïë\n",
    "‚ïë     Config:          {OUTPUT_DIR}/config.json                       ‚ïë\n",
    "‚ïë     Metrics:         {OUTPUT_DIR}/metrics.json                      ‚ïë\n",
    "‚ïë                                                                              ‚ïë\n",
    "‚ïë  ‚úÖ Listo para meta-learner                                                  ‚ïë\n",
    "‚ïë                                                                              ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timesfm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
